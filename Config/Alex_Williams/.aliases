# -*-Sh-*- <-- tells emacs what kind of syntax highlighting to use

# ---------------------
# "Unofficial bash strict mode: " #!/bin/bash
# set -euo pipefail
# IFS=$'\n\t'
# ---------------------
if [ -f ${BINF_CORE_WORK_DIR}/Code/alexgw/aliases-server-config ]; then
    source ${BINF_CORE_WORK_DIR}/Code/alexgw/aliases-server-config
elif [ -f ~/.aliases-server-config.sh ]; then
    source ~/.aliases-server-config.sh
fi

if [ -f ~/.local-config-no-git.sh ]; then
    source ~/.local-config-no-git.sh
else
    echo -e "[:HEY:] Since no ~/.local-config-no-git.sh was found, SSH commands, server shortcuts, and tmux coloring may fail. You may want to MANUALLY copy this file over from another machine."
fi

if [[ "${USER}" == "alexgw" ]] || [[ "${USER}" == "awilliams" ]] || [[ "${USER}" == "alexwilliams" ]] || [[ "${USER}" =~ willia* ]]; then
    IS_ALEX_ACCOUNT=1
    ## SAFER 'rm' COMMANDS --only applies to the user(s) above.
    ## Below: aliases the command "rm" to actually run the script "trash.pl," which moves files to a trash directory in /tmp.
    ## They can be recovered by just copying them back.
    ## Use the "checktrash" command to find the trash directory if you forget where it is.
    ## Below: the command to list the top-level contents of the trash directory. You may have to look through the files here if you want to recover something you just accidentally deleted. Beware, files don't last long in /tmp\!
    alias checktrash='mkdir -p /tmp/${USER}/Trash/ && echo -e "Contents of /tmp/${USER}/Trash:" && ls /tmp/${USER}/Trash/'
    alias emptytrash='mkdir -p /tmp/${USER}/Trash/ && /bin/rm -rfv /tmp/${USER}/Trash/ && echo -e "Emptied the trash directory (/tmp/${USER}/Trash)" ;'
    alias rm='trash.pl' ## <-- Note that the "real" rm can still always be invoked by '/bin/rm'
    OUR_RM='trash.pl'
else
    IS_ALEX_ACCOUNT=0
    OUR_RM=rm
fi

echo -e "${a_echo_color}[:FYI:] Loading bash-aliases...${a_end_color}"

ERRTT="[:ERR:]"  # Set to highlight in iTerm

LITERAL_TAB='	'
CODE_DIRS_FOR_GREPPING="${TIME_FOR_SCIENCE_DIR}" # <-- Multiple **space-delimited** directories can be here
if [[ -n "${isMac}" ]] ; then ## It *is* a mac!
    function g { ## Searches certain source directories, and the current directory (and any subdirectories!)
	`which grep` --color=always --ignore-case --recursive --extended-regexp --exclude="Mothballed" --exclude=".git" --exclude=".hg" --exclude="CVS" --exclude="[bB]ackup*" \
		     --exclude="*.gsheet" --exclude="*.gdoc" --exclude="*.gslides" --exclude="*.html" --exclude="Annotation*.txt" \
	    "$@" ${CODE_DIRS_FOR_GREPPING} ./
    }
    alias zcat="gzip --decompress --stdout" # The Mac doesn't have the gzip-handling zcat (AKA gzcat)
else
    ## Not a Mac!
    function g {     ## Annoyingly, grep is different between mac/unix
	cmd=(	grep --color=always -T \
	    --ignore-case \
	    --recursive \
	    --extended-regexp \
	    --line-number --with-filename \
	    --exclude-dir="Mothballed" --exclude-dir="\.hg" --exclude-dir="\.git" --exclude-dir="CVS" --exclude-dir="[bB]ackup*" \
	    --binary-files=without-match \
	    --include=*.R --include=*.pl --include=*.pm --include=*.py --include=*.sh --include=*.py
	    --exclude=*.html --exclude=*.gslides --exclude=*.gdoc --exclude=*.gsheet )
	cmd=( "${cmd[@]}" "$@" ${CODE_DIRS_FOR_GREPPING} ) # some weird array format
	"${cmd[@]}" # <- Actually execute. See http://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters
    }
fi


function template() {
    if [[ "$#" != 1 ]] ; then echo -e "$ERRTT 'template ___' requires EXACTLY ONE file/directory suffix!"; return 1; fi
    EX=$1
    case "${EX}" in 
        [Mm]ak*) SRC=`which template.mak`; DEST="Makefile" ;;
        [Rr][Mm][Dd]*) SRC="${TIME_FOR_SCIENCE_DIR}/Lab_Code/R/template.Rmd"          DEST="./template.Rmd"        ;;
        [Rr])          SRC="${TIME_FOR_SCIENCE_DIR}/Lab_Code/R/template.R"            DEST="./template.R"          ;;
        py2)           SRC="${TIME_FOR_SCIENCE_DIR}/Lab_Code/Python/template-py2.py"  DEST="./template_python2.py" ;;
        py | py3)      SRC="${TIME_FOR_SCIENCE_DIR}/Lab_Code/Python3/template.py"     DEST="./template_python3.py" ;;
        bash | sh)     SRC="${TIME_FOR_SCIENCE_DIR}/Lab_Code/Shell/template.sh"       DEST="./template.sh" ;;
        *) echo -e "$ERRTT template did not understand what type of file you requested: the extension you supplied was <${EX}>." ; return 1 ;;
    esac
    if [[ -e "${DEST}" ]]; then
        echo "$ERRTT Refusing to overwrite the existing file '${DEST}'."
        return 1
    else
        cp -i "${SRC}" "${DEST}"
        echo "[:OK:] Copied a new '${EX}' template to <${DEST}>."
    fi
}

function agw_aaa_autocomplete_capture() {
    echo "This is alphabetically first in the 'agw_' commands."
    echo "It is just to capture any inadverent tab completion."
}

function agw_split_files_into_new_subdirectories_in_batches_of_100() {
    echo "Moving (a presumably large quantity of) files in this directory into subdirectories named 'subdir###'."
    n=0;
    for f in *; do
	d="subdir$((n++ / 100))";
	mkdir -p "$d";
	mv -- "$f" "$d/$f";
    done
}

function agw_preview_app_delete_its_cache_files() {
    echo "Deleting an assortment of various cache-related files that can cause Apple Preview.app to get confused and refuse to open files with various 'cannot open' errors. Still occasionally necessary as of August 2021."
    rm -rf ~/Library/Containers/com.apple.Preview
    rm -rf ~/Library/Saved\ Application\ State/com.apple.Preview.savedState
    rm -rf ~/Preferences/com.apple.Preview.LSSharedFileList.plist
    rm -rf ~/Library/Preferences/com.apple.Preview.SandboxedPersistentURLs.LSSharedFileList.plist
    rm -rf ~/Library/Caches/com.apple.Preview
}

function delete_mp3s_where_an_m4a_file_exists_with_the_otherwise_same_name() {
    echo "May be dangerous. Attempts to delete any mp3 files where there is ALSO an (ideally uncompressed?) m4a file with the same name."
    echo "Basically just finds all the m4a files and then attempts to delete the (perhaps present) .mp3 version of the file."
    echo "The only real trick to this is that it can be non-obvious how to craft a command that handles spaces/quotation marks/etc."
    find . -depth -type f -name "*.m4a" -exec sh -c 'rm -- "${1/.m4a}.mp3"' _ '{}' \;
}

function agw_AMPDevicesAgent_iOS_kill_fixes_sync_operation_is_still_in_progress() {
    echo "This should solve the <<Finder can't quit because an operation is still in progress on an iOS device>> problem (iOS / iPhone syncing taking forever)."
    killall -9 AMPDevicesAgent
}

function agw_quarantine_deleter() {
    if [[ "$#" -eq 0 ]] ; then echo -e "[:ERR:] Please provide at least one file/directory to recursively delete quarantine flags in." ; return 1; fi
    echo "RECURSIVELY deleting quarantine flags in the supplied path and any subdirectories."
    sleep 4
    xattr -dr com.apple.quarantine $*
}
    
function colorssh() {
  tabc SSH
  ssh "$*"
  tabc
}

function backup() {
    bdir="./Backups"
    # Back up one or more files into a <$bdir> backup subdirectory in this same directory
    if [[ $# -eq 0 ]]; then
        echo "E0001 99: You must specify AT LEAST ONE file to copy to the $bdir directory." >&2
        return 1
    fi
    if [[ ! -d "${bdir}" ]]; then
        echo "E0100 99: A <$bdir> directory must exist here." >&2
        return 100
    fi
    seconds=$(date +%s) # time in unix seconds
    for filename in "$@"; do
        bak="${bdir}/${filename}.${seconds}.bak"
        set -x # Turn on echoing of the following commands
        cp "$filename" "$bak"
        { set +x; } 2>/dev/null # Turn off echoing without echoing the "set +x"
    done
}

function agw_dupe_delete_this_folder() {
    echo "Delete exact-duplicate files. Just run `fdupes` instead. (brew install fdupes, then fdupes -dN .)"
}

function sleepmin() { # Sleep N minutes
    if [[ "$#" != 1 ]] ; then echo -e "$ERRTT 'sleepmin' requires a number of minutes to sleep! One argument."; return 1; fi
    seconds_to_sleep=$(($1 * 60))  # Bash math!
    \sleep "$seconds_to_sleep"
}

function agw_cmd_exists() { # Check if a command exists
    #command -v "$1" &>/dev/null #|| { echo >&2 "I require foo but it's not installed.  Aborting."; exit 1; }
    type "$1" &> /dev/null
    # Alternative approach: command -v "scutil" > /dev/null # <-- Note: we check the exit code from this ("$?") below
    # And then check the exit code:   HAS_PROGRAM=$((1-$?)) # $? = 0 means the above command SUCCEEDED
    # or try: if [[ -n `which exa 2> /dev/null` ]] ... # Usage example: if agw_cmd_exists "exa" && [ "$isMac" == "1" ] ; then ...
    # note that it returns 0 on success, and zero is TRUE in bash!
    # usage: if agw_cmd_exists "command"; then do something; fi
}

function hunt { # cd into a directory with a script
    echo -e "Hunting the most dangerous game of all: cd-ing the directory where \"$(basename $(which $1))\" lives..."
    THEDIR=$(dirname $(which $1 | head -n 1))
    cd "$THEDIR"
    echo -e "Moved into directory <$THEDIR>."
}

function cto { # Like CD, but lets you go to a FILE as well.
    if [[ "$#" != 1 ]] ; then echo -e "$ERRTT 'cto' requires EXACTLY ONE file/directory name!"; return 1; fi
    if [[ -d "$1" ]] ; then cd "$1" ; return 0; fi # it's already a directory, change to it!
    cd "$(dirname "$1")"
}

function c() { # Like CD, but lets you go to a FILE as well.
    if [[ "$#" != 1 ]] ; then echo -e "$ERRTT 'cto' requires EXACTLY ONE file/directory name!"; return 1; fi
    if [[ -d "$1" ]] ; then cd "$1" ; return 0; fi # it's already a directory, change to it!
    cd "$(dirname "$1")"
}

alias igv='/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/bin/java -jar /Applications/IGV_2.4.10.app/Contents/Java/igv.jar'

function witch() { # Shows the versions for all on-your-path verisons of software.
    # Takes ONE required argument: the program name. Can also take a SECOND argument, which is the actual command to run to check versions (if it isn't 'version')
    # Example:   witch emacs
    #               This works, because 'emacs --version' is how you get the version
    # But for Java, you have to say the following:
    # Or:        witch java -version   (some programs are dumb and do not support --version)
    #               Since the default "--version" doesn't work with Java
    VER_STR=" --version "
    if [[ "$#" == 0 ]]; then
	echo -e "ERR: You need to specify a program to check the versions of. Example:  witch emacs --version ."; return 1;
    fi
    if [[ "$#" == 1 ]] ; then echo -e "witch: Using default '--version' to check for program version..."; fi
    if [[ "$2" != "" ]]; then
	# $2 was actually defined...
	VER_STR="$2"
	#echo -e "ERR: You need to specify a version-obtaining-argument like '--version' to the 'witch' command";
	#echo -e "     Example:   witch emacs --version   or java -version (Java doesn't support --version)";
	#return 1;
    fi
    for f in $(which -a "$1"); do echo -en "\n${a_echo_color}$f${a_end_color} --> "; eval "$f $VER_STR" | head -n 2; done; echo ""
}

function delve() { # or 'dv'
    ## Finds, in any subdirectories, any files that have a name that matches the input text. Sort of like a poor man's "locate"
    find . -iname "*$@*"
}

function agw_find_small_files() {
    echo "Finding files (of any type) that are under 50 kilobytes."
    echo "Add `-delete` to the command below to actually *delete* the files."
    echo "Running:  find -E . -type 'f' -size -50k "
                    find -E . -type 'f' -size -50k
}

BRACKET_OPEN='{'
PAREN_OPEN='('
TAB='	'
DLR='\$'
DQ='\"'
BS='\\'
NL="\$'\\n'" # newline!

# Shell navigation/commands

#pushd() { builtin pushd "$@" > /dev/null; } ## <-- make it so that pushd doesn't print the stack every single time

#hasExa=agw_cmd_exists "exa"

#if [[ "$hasExa" && "$isMac" ]]; then # check for 'exa' -- the 'ls' replacement, but only on the Mac---it's slow on Rigel!

#LS_MAX_ENTRIES=200
#alias l2='echo -e "$(exa --color=always)"'

SNEK="\xF0\x9F\x90\x8D"
if agw_cmd_exists "exa" && [ "$isMac" == "1" ] ; then
    # 'exa' is a 'ls' replacement
    function lsw {
	echo -e "${a_banner_color}> >" $(pwd -P) "${a_end_color}"
	if [[ -e ".readme" ]]; then
	    echo -e -n "${a_dirinfo_color}"
	    cat ".readme" | fold -s -w 80 | sed -e "s/^/$(echo -ne ${SNEK})  /"
	    echo -e -n "${a_end_color}"
	    #printf "\xF0\x9F\x90\x8D" # a snake
	fi
	exa --color=auto
    }
    alias ls='exa --color=auto'
    alias ll='exa -l -a --color=auto'
    
elif agw_cmd_exists "exa" ; then
    # Not the mac, but apparently exa is still installed.
    alias ls='exa --color=auto'
    alias ll='exa -l -a --color=auto'
else
    if [[ "$color_prompt" && "$isMac" ]]; then
	AGW_LS_OPT=' -F -G ' #-@ ' ## Mac: Color option is -G. Also, on a Mac, show the extended attributes (-@)
	AGW_LL_COLOR_ALWAYS=${AGW_LS_OPT} # same as above, on the mac
    elif [[ "$color_prompt" && (-z "$isMac") ]]; then
	AGW_LL_COLOR_ALWAYS=' --indicator-style=slash --color=always '
	AGW_LS_OPT=' --indicator-style=slash --color=auto ' ## Ubuntu: color is --color=auto
    else
	AGW_LS_OPT=''
    fi

    if [[ "$isMac" ]]; then
	AGW_MACEXT=' -@ ' # Mac extended attributes
    else
	AGW_MACEXT=' '
    fi

    alias ls="/bin/ls               ${AGW_LS_OPT} "
    function llfunction {
	export CLICOLOR_FORCE=1
	#/bin/ls -l -h -A -F ${AGW_LL_COLOR_ALWAYS} "$@" | perl -pe 's!(.*)!${1} WHAT [:H:]!'
	#LS_ENDCOLOR=$(echo '[0m')
	#|WHATDIR${LS_ENDCOLOR}\/
	/bin/ls -l -h -A -F ${AGW_LL_COLOR_ALWAYS} "$@" | perl -pe "s!(.*)(
P_L_A_C_E__H_O_L_D_E_R
|[.]aliases
|[.]atom.*[/]\$
|[.]bash_history
|[.]bash_logout
|[.]bash_profile
|[.]bashrc
|[.]bash_sessions
|[.]biojs_templates
|[.]cache
|[.]cache
|[.]CFUserTextEncoding\$
|[.]conda.*[/]\$
|[.]condarc
|[.]config
|[.]cups
|[.]dbfseventsd.*[=]\$
|[.]dbus.*[/]\$
|[.]DocumentRevisions-V100.*[/]\$
|[.]dropbox
|[.]DS_Store
|[.]emacs[.]?[d]?
|[.]file\$
|[.]fontconfig.*[/]\$
|[.]fseventsd.*
|[.]gconf.*[/]\$
|[.]gitconfig
|[.]gnome2.*[/]\$
|[.]gnupg
|[.]GSeqWeb.*[/]\$
|igv.*[/]
|[.]inputrc
|[.]InstallAnywhere.*[/]\$
|installer[.]failurerequests
|[.]iterm2.*[/]\$
|[.]ipython.*[/]\$
|[.]iterm2_shell_integration.bash
|[.]jalview_properties
|[.]jenv
|[.]jswingreader
|[.]jupyter.*[/]\$
|[.]kshrc
|[.]lesshst
|[.]local
|[.]lsbatch.*[/]\$
|[.]matplot.*[/]\$
|[.]modules.*->
|[.]modulesbeginenv
|[.]mozilla.*[/]\$
|[.]oracle_jre_usage
|[.]OSInstallerMessages\$
|[.]PKInstallSandboxManager.*[/]\$
|[.]python_history
|[.]pylint.*[/]\$
|[\d\dm|\s][.]R.*[/]\$
|[.]Rhistory.*\$
|Rlib.*[/]
|[\d\dm|\s][.]RData
|[.]rstudio.*[/]\$
|[.]rstudio-desktop
|[.]screenrc
|[.]serverauth
|[.]Spotlight-V100.*[/]\$
|[.]ssh.*[/]
|[\d\dm|\s]Shares.*[/]\$
|[.]subversion
|[.]sqlwork.*[/]\$
|[.]t_coffee
|TimeForScience.*[/]
|[.]tmux[.]conf
|[.]Trash
|[.]UGENE_downloaded.*[/]
|[.]UGENE_files.*[/]
|[.]vamsas
|Velocity_ini[.]txt
|[.]viminfo
|[\d\dm|\s]vc.*[/]
|[.]vol.*/\$
|[.]VolumeIcon.icns
|[.]vscode.*[/]\$
|[.]wget-hsts
|[.]Xauthority
|[.]zoomus
)(.*)\$!\$1\$2\$3\ [:H:]!"xx
	unset CLICOLOR_FORCE
    }
    alias ll=llfunction
    #alias ll="/bin/ls   -l -h -A -F ${AGW_LS_OPT} "
    alias lmac="ll ${AGW_MACEXT} " # mac-specific with EXTENDED attributes (-@)
fi

alias l='ls'
alias lo='ll -r -t' # Depends on "ll" already being defined above. List by TIME so that NEWEST files are at the bottom
alias p='pwd -P'
alias ..='cd ..'
#alias ../=='cd ..'
#alias ../../='cd ../../'
#alias ..='pushd ..'
alias res='source ~/.bashrc ; source ~/.bash_profile'
alias mv='mv -i'
alias cp='cp -i'
alias kpk='exit'
alias diffdir='diff -rq' ## diff on directories
alias t="transpose.pl"
alias crone="EDITOR=emacs crontab -e"

# Turn on/off crashplan. No longer necessary.
#alias recrash='sudo launchctl unload /Library/LaunchDaemons/com.crashplan.engine.plist; sleep 1; sudo launchctl load /Library/LaunchDaemons/com.crashplan.engine.plist'

#alias tc='randomize_terminal_color.pl -cycle'
#alias cd='pushd'
#alias b='popd'

## "showoff" makes it so that everyone can a+rx any directories and a+r any files.
## Note that we don't want to make it so everyone can execute *files* necessarily, just folders
## ("Executing" a folder means you can "ls" it and see what's inside.)
function showoff {
    if [ "$#" -eq 0 ]; then echo -e "$ERRTT Usage: 'showoff' requires path argument(s)! Example: 'showoff ./' for the current directory."; return 1; fi
    for var in "$@"; do echo -e "[:FYI:] showoff is now running 'chmod -R' to allow ANY USER to browse \"$var\""; chmod -R a+rX "$var"; done
}
function sudoshowoff {
    if [ "$#" -eq 0 ]; then echo -e "$ERRTT Usage: 'sudoshowoff' requires path argument(s)! Example: 'sudoshowoff ./' for the current directory."; return 1; fi
    for var in "$@"; do echo -e "[:FYI:] sudoshowoff: Using sudo chmod -R to allow ANY USER to browse \"$var\""; sudo chmod -R a+rX "$var"; done
}

## Mac-specific commands:
SETFILE_LOCATION=/Applications/Xcode*/Contents/Developer/Tools/SetFile
alias invisible='chflags -h hidden'  #alias invisible='chflags hidden' #$SETFILE_LOCATION -P -a V' ## Mac-only: Make a file/folder invisible to the Finder
alias visible='chflags -h nohidden' #alias visible='chflags nohidden' #$SETFILE_LOCATION -P -a v' ## Mac-only: opposite of "invisible"
alias lock='chflags uchg'     # Mac: lock file
alias unlock='chflags nouchg' # Mac: unlock file

alias clearicon='$SETFILE_LOCATION -P -a c' ## clear mac custom icons. Useful for images that have old custom icons.
alias version="lsb_release -a" # Tells you which version of Ubuntu you are running!
alias vscode="code"
alias bb='bbedit' # BBEdit. Must be installed from the BBEdit "install terminal integration" command
alias vs='code' # Visual Studio Code. Must be installed manually!
#function bb() {
#    open -a "/Applications/BBEdit.app" "$@"
#}

function mac2unix { # Convert a Mac-style line-ending file to a UNIX one. Useful for when you save a file in Excel and then UNIX won't read it.
    if [[ -f "$1" ]] ; then cat "$1" | tr '\r' '\n' ## if a filename is passed in, then auto-cat that file
    else tr '\r' '\n' ; fi ## otherwise it's probably part of a cmdline pipe
}

function dos2unix { # Convert a Windows-style line-ending file to a UNIX one. Useful for when you save a file in Excel and then UNIX won't read it.
    if [[ -f "$1" ]] ; then cat "$1" | tr -d '\r' ## if a filename is passed in, then auto-cat that file
    else tr -d '\r'; fi ## otherwise it's probably part of a cmdline pipe
}

## Utility commands

if [[ -f ~/.agw_cfg/prefer_screen ]]; then
    echo -e "[:FYI:] Preferring SCREEN over tmux: aliasing 'rr' to screen."
    alias rr='screen -xR -U' #"screen -xR -U" ## Reconnect to the previous screen, or make a new one if there isn't one already
    ## -U: "screen understands UTF8"
else
    alias rr='tmux attach || tmux new' #"screen -xR -U" ## Reconnect to the previous screen, or make a new one if there isn't one already
fi

alias wcl='grep -c ""' # actually gives you the NUMBER and not the filename
alias sortt='sort -t "	"' # sort with tab as separator
alias sortg='sort -g -t "	"' # sort NUMBERS, with tab as separator

#if [[ -f /usr/local/bin/emacs ]]; then
#if [[ -n "$isMac" ]]; then
#    alias e='emacs --no-splash -nw ' # use 'brew install emacs' to update emacs
#else
alias e="emacsclient -nw  -c --alternate-editor=\"\"" # "As a special exception, if command is the empty string, then emacsclient starts Emacs in daemon mode (as emacs --daemon) and then tries connecting again."
#alias e2="LANG='en_US.UTF-8' bash -c 'emacsclient -nw  -c --alternate-editor=\"\"'" # "As a special exception, if command is the empty string, then emacsclient starts Emacs in daemon mode (as emacs --daemon) and then tries connecting again."
#fi

function ql_files() {
    # List any offending quicklook files. You may also want to count them with "wc -l" at the end.
    lsof | grep Quick | grep -v -e /System -e /Library -e /private -e /dev -e /usr -e KQUEUE -e cwd
}

function ql_kill() {
    # Kill the weird quicklook processes that sometimes accumulate and make the UI laggy (and maybe this is what makes WindowServer take up all the CPU?) for unknown reasons.
    echo "[:FYI:] Killing QuickLookSatellite: killall -9 -v QuickLookSatellite"
    sudo killall -9 -v QuickLookSatellite
    echo "[:FYI:] Killing QuickLookUIService: killall -9 -v QuickLookUIService"
    sudo killall -9 -v QuickLookUIService
    echo "[:FYI:] Resetting cache: qlmanage -r cache"
    sudo qlmanage -r cache
}


#alias t='transpose.pl -q'
alias tattle="echo -e -n '$a_status_color'; ps aux | tail -n +2 | sort --reverse -k 3,3 | head -n 5 | perl -p -e 's/[ ]+/\t/g' | cut -f 1,3,4,11 | cap.pl 'USER,CPU,MEM,TASK' | sheet.pl --color=always \
--ht=75 --trunc=60 | tail -n +2 ; echo -e -n '$a_end_color'"

# GNU Make-related
alias make='make --warn-undefined-variables --print-directory'
alias mcm='make clean && make'
function remake { ## Lets you type "remake" to remove a file and then use GNU make to try to re-generate it
    ${OUR_RM} "$@" ; make --warn-undefined-variables --print-directory "$@"
}

function rel { ## Remake and then view
    ${OUR_RM} "$@" ; make --warn-undefined-variables --print-directory "$@" ; s "$@"
}

# Aliases related to COLOR GREP
alias grepi='grep   -i' # <-- runs fast if "LANG=C" is the specified language
alias grepc='grep   --color=always --with-filename --line-number'
alias egrepc='egrep --color=always --with-filename --line-number'
alias fgrepc='fgrep --color=always --with-filename --line-number'



function git-diff-commits-in-branch-1-but-not-branch-2() {
    # Shows commits in the first branch but NOT the second one. Order matters!
    if [[ "$#" != 2 ]];  then echo -e "$ERRTT Please provide TWO branch names. Order matters."; return 1; fi
    b1="$1"
    b2="$2"
    \git cherry --verbose "${b1}" "${b2}" | grep "^\+"
}

function git-grep-all-local-branches() {
    \git grep --color=always "${1}" $(git rev-list --all)
}

function git-last-modified {
    if [[ "$#" != 1 ]]; then echo -e "$ERRTT 'git-last-modified' needs exactly one filename to be supplied!"; return 1; fi
    file=$1
    git --no-pager log -1 --pretty="format:%ci" "$file"; echo "${LITERAL_TAB}${file}"
}


alias gitgrepall="git branch -a | tr -d \* | sed '/->/d' | xargs git grep" # greps ALL branches
function gitbranchall {
    for branch in `git branch -r | grep -v HEAD`;do echo -e `git show --format="%ci %cr" $branch | head -n 1` \\t$branch; done | sort -r
}

function git-grab-hash {
    if [[ "$#" != 1 ]]; then echo -e "$ERRTT 'git-grab-hash' needs exactly one hash argument!"; return 1; fi
    echo "[:FYI:] Grabbing remote hash as new local branch..."
    echo "[:OK:]       --> git checkout \"$1\"  -b \"local-$1\""
    (set -x; git checkout "$1"  -b "local-$1"; )
}

# git-related aliases
alias gitours="git checkout --ours"
alias gitcom="git commit -a -m 'Changes committed by ${USER}'"
alias git-list="git ls-tree -r master --name-only" # show currently tracked files
alias gits="git status"
alias git-stat="git ls-files --modified --deleted"
alias git-diff="git difftool"
#alias git-pull-master-without-changing="echo 'Updating <master> without changing branches.'; git fetch origin master:master"
alias git-pull-unstable-without-changing="echo 'Updating <unstable> without changing branches.'; git fetch origin unstable:unstable"

function git-diff-origin() {
    # Compare current git BRANCH_NAME to the (probably) same-named remote branch "origin/BRANCH_NAME"
    # Theoretically, the branch could have a different remote 'origin' being tracked. But that's rare in practice.
    cur_branch=$(\git branch --show-current)
    echo "Diffing against same-named origin branch... git diff '${cur_branch}' 'origin/${cur_branch}'"
    \git diff "${cur_branch}" "origin/${cur_branch}"
}

function git-push-origin-initial-setup() {
    # git push --set-upstreadm origin SAME_NAME_BRANCH
    cur_branch=$(\git branch --show-current)
    (set -x; \git push --set-upstream origin "${cur_branch}"; )
}

#if agw_cmd_exists "git" ; then
# Git aliases have been moved to mac_initial_setup.sh
#fi

function sciencecom {
    # If you want to NOT interact remotely (for example, this is the 'most upstream' repo with nothing to push to) then pass in NO_REMOTE as argument 2.
    if [[ -f ~/.ssh/id_rsa_timeforscience ]]; then ssh-add ~/.ssh/id_rsa_timeforscience; fi
    if [[ ! -d "$1/.git" ]] ; then echo -e "[:HEY:] No $1 git repository exists on this machine."; return 1; fi
    if [[ "${2}" == "NO_REMOTE" ]]; then SHOULD_REMOTE=0; else SHOULD_REMOTE=1; fi
    #echo -e "[:FYI:] Committing GIT changes locally."
    git --git-dir="$1/.git" --work-tree="$1" commit -a -m "Committing changes from ${USER}"
    if [[ "${SHOULD_REMOTE}" == 1 ]]; then
	#echo "Should remote COMMIT --> YES PLEASE ${SHOULD_REMOTE}"
	echo -e "Pushing GIT changes to remote master."
	git --git-dir="$1/.git" --work-tree="$1" push origin master ;
    fi
}

function scienceup {
    # If you want to NOT interact remotely (for example, this is the 'most upstream' repo with nothing to pull from) then pass in NO_REMOTE as argument 2.
    if [[ ! -d "$1/.git" ]] ; then echo -e "[:HEY:] No $1 git repository exists on this machine."; return 1; fi
    if [[ "${2}" == "NO_REMOTE" ]]; then SHOULD_REMOTE=0; else SHOULD_REMOTE=1; fi
    #echo "Should remote --> ${SHOULD_REMOTE}"
    echo -e "[:FYI:] Committing any local GIT changes before syncing with remote master."
    git --git-dir="$1/.git" --work-tree="$1" commit -a -m "Committing any local changes from ${USER} before sync with remote server." ;
    if [[ "${SHOULD_REMOTE}" == 1 ]]; then
	#echo "Should remote pull --> YES PLEASE ${SHOULD_REMOTE}"
	echo -e "[:FYI:] Pulling GIT changes from remote master."
	git --git-dir="$1/.git" --work-tree="$1" pull ;
    fi
}

function scienceboth() {
    if [[ -d "$1" ]]; then
	echo -e "${REVERSE}$1:${a_end_color}"; scienceup "$@" && sciencecom "$@"
    fi

}
function science() {
    scienceboth "${TIME_FOR_SCIENCE_DIR}"
}

AGW_SSH_OPTIONS=" -o ServerAliveInterval=30 -C "

function rmd {
    if [[ "$#" != 1 ]]; then echo -e "$ERRTT the 'rmd'-ify command needs exactly one filename argument."; return 1; fi
    FILE_TO_RENDER=$1
    if [[ ! ${FILE_TO_RENDER} =~ .*Rmd ]]; then echo -e "$ERRTT Your file to render, <${FILE_TO_RENDER}>, did not end in '.Rmd'!!"; return 1; fi
    if [[ ! -f "${FILE_TO_RENDER}" ]]    ; then echo -e "$ERRTT Your file to render, <${FILE_TO_RENDER}>, did not appear to exist!"; return 1; fi
    NON_SYMLINKED="${FILE_TO_RENDER}.alias.tmp.Rmd" # R markdown handles aliases in a super annoying fashion
    if [[ ! -f "${NON_SYMLINKED}" ]]    ; then echo -e "$ERRTT We could not copy the file to a non-symlinked location!"; return 1; fi
    WORKDIR=`dirname ${FILE_TO_RENDER}`
    \cp -f --dereference "${FILE_TO_RENDER}" "${NON_SYMLINKED}"
    rmd_cmd="Rscript -e 'library(rmarkdown); rmarkdown::render(\"${NON_SYMLINKED}\", output_dir=\"${WORKDIR}\", knit_root_dir=\"${WORKDIR}\");'  "
    echo "[:OK:] Running this command: " ${rmd_cmd}
    eval ${rmd_cmd}
    \rm "${NON_SYMLINKED}"
}

alias agwcom='ssh ${AGW_SSH_OPTIONS}   ${AGWCOM_USERNAME}@${AGWCOM_URL}'

alias nami=' ssh ${AGW_SSH_OPTIONS} -i ~/Dropbox/metaquery-dev.pem bitnami@52.53.252.164'
alias rig=' ssh ${AGW_SSH_OPTIONS}     ${ACC_U}@${RIG_IP}'
RIG_SAND_IP=${RIG_IP/./-sandbox.} # add "-sandbox" before the first dot

## LESS +++++++++++++++++
export PAGER=less
[[ -x /usr/bin/lesspipe ]] && eval "$(SHELL=/bin/sh lesspipe)" # make "less" friendly for non-text input files
export LESSOPEN="|${TIME_FOR_SCIENCE_DIR}/Config/1_Shell_Config/lesspipe_basic.sh %s"
# lesspipe_basic.sh casues less to transparently decompress gzipped files before showing them. Old:  '|/usr/bin/lesspipe.sh %s'
alias magicless='env LESSOPEN="|${TIME_FOR_SCIENCE_DIR}/Config/1_Shell_Config/lesspipe_advanced.sh %s" /usr/bin/less -S --RAW-CONTROL-CHARS -f --IGNORE-CASE'
# It's less, but it "magically" handles gzipped files and automatically runs ".tab" files through sheet.pl
# Note that this only magically happens if the files are passed in on the command line--otherwise you have
# to use "ssf" to force sheet.pl to be run (if a file is passed through a pipe, then it won't be run through
# sheet.pl unless you say "cat something | ssf"
## LESS +++++++++++++++++

# Plain "sn" doesn't run anything through sheet.pl, but it *does* handle gzipped files
alias sn="/usr/bin/less -S --LINE-NUMBERS --status-column --RAW-CONTROL-CHARS -f --IGNORE-CASE"
alias sweep="${OUR_RM} *.tmp *.temp" ## Sweep out the .tmp files
alias htop="htop --sort-key PERCENT_CPU"

#function playnicely {
#    # Takes one argument.
#    # requires sudo privileges. Takes one argument, the username to make play nicely:
#    # example: playnicely THEUSERNAME
#    SSS=$(ps -fu "$1" | perl -pe 's/[ ]+/\t/g' | cut -f 2 | tail -n +2);
#    for pid in $SSS ; do echo 'ionice-ing and nice+15-ing process ID' $pid; sudo ionice -c3 -p $pid ; sudo renice +15 -p $pid ; done 
#}
#
#function disknicely {
#    # Takes ZERO arguments. Just for making the 'nfsd' processes less annoying.
#    SSS=$(ps -eopid,uid,cmd | grep "nfsd" | grep -v "grep" | perl -p -e "s/^[ ]+//" | cut -d " " -f 1) # Get any 'nfsd' processes
#    for pid in $SSS ; do echo 'ionice-ing and nice+15-ing the NFS process ID' $pid; sudo ionice -c3 -p $pid ; sudo renice +15 -p $pid ; don#e 
#}

function s() { ## <-- this needs to come BEFORE the other things that use less!
    magicless --LINE-NUMBERS --status-column "$@"  # "s" uses "magicless" to run files through sheet.pl
}

function acol() {
    column -s $'\t' -t # columns delimited by tabs! Like 'sheet.py'
}

function acols() {
    s "$@" | acol | s # Pipe it through magicless, then 'acol', then magicless AGAIN. Like a poor man's sheet.py (way faster, though)
}

function sf() {
    # Forces sheet.pl to be called ("SS Force sheet.pl").  Can only view ONE file, unlike "ss"
    /usr/bin/less --RAW-CONTROL-CHARS "$1" | sheet.pl --color=always | s
}

function s1() {
    # like sf, but shorter columns # Can only view ONE file, unlike "s"
    /usr/bin/less --RAW-CONTROL-CHARS "$1" | sheet.pl --color=always --trunc=15 | s
}

function s2() {
    # like sf, but shorter columns # Can only view ONE file, unlike "s"
    /usr/bin/less --RAW-CONTROL-CHARS "$1" | sheet.pl --color=always --trunc=25 | s
}

function v() {
    # Use sheet.py to view a file or list of files. These files cannot be gzipped, however.
    # Must be python2.6, and not python 3, currently
    `which sheet.py` "$@"
}

alias huh='cat <(declare -f) <(alias)' ## uses bash subshells to show everything that is defined
#alias backoff='${TIME_FOR_SCIENCE_DIR}/Config/Alex_Williams/unix_scripts/crashplan-backup-mod.sudo.sh off'
#alias backon='${TIME_FOR_SCIENCE_DIR}/Config/Alex_Williams/unix_scripts/crashplan-backup-mod.sudo.sh on'

function n() { # show colors
    # ${cpre} is defined in my ~/.bashrc (usually as \033)
    echo -e  "${cpre}[30m black ${cpre}[31mred ${cpre}[32mgreen ${cpre}[33myellow ${cpre}[34mblue ${cpre}[35mmagenta ${cpre}[36mcyan ${cpre}[37mwhite"
}

## Maczip zips things on the mac WITHOUT including the dot files and .DS_Stores
function maczip() {
    if [[ "$#" == 1 ]] ; then theCompressedBasename="$1"; ## If you pass it one argument, then that argument will also be the filename
    else theCompressedBasename="Archive_zipped_without_Mac_files";
    fi
    theCompressedBasename=`echo "$theCompressedBasename" | sed "s/[/]//"` ## Let's remove any slashes from the name of the *output*. That way if someone types "mini MyDir/" it will generate "MyDir.zip" and NOT "MyDir/.zip"
    if [[ -f "${theCompressedBasename}.zip" ]] ; then
	echo -e "$ERRTT mini: HALTING: Cannot create a new archive---perhaps ${theCompressedBasename}.zip already exists.\nThis is forbidden--you cannot create the archive to overwrite the current archive!!! Delete it and compress again." ;
	return 1;
    else
	if [[ "$#" -gt 1 ]] ; then echo -e "[:FYI:] Adding a total of $# files to the new zip archive, which OMITS Mac-specific files (e.g. .DS_Store)." ; fi
	for var in "$@"; do echo -e "[:FYI:] * Archiving $var --> ${theCompressedBasename}.zip" ; done
	echo -e "[:FYI:] ZIP command running now..." ;
	zip -9 -r -X --symlinks --exclude "*.DS_Store" --exclude="*~" --exclude="\$*" "${theCompressedBasename}.zip"  $@;
	echo -e "[:FYI:] Compression complete." ;
    fi
}

function xztar() {
    # Tar and xzip in one place so you don't have to remember the syntax. Or you could just remember the syntax!
    #if [[ "$#" != 1 ]]; then echo -e "$ERRTT xztar can only accept ONE argument! One directory or file to tar/xz."; return 1; fi
    F_TAR=$(basename $1).tar
    F_TAR_XZ=$(basename $1).tar.xz
    echo -e "Note that xz is slow! So be prepared for it to take forever. Creating this archive: ${F_TAR_XZ}"
    tar -cvf "${F_TAR}" "$@"
    time xz --threads=0 -9 "${F_TAR}"
}

## Mini is a function that creates a .bz2 archive of whatever you pass into it.
## It works in a manner similar to right-clicking and zipping a file on the Mac.
## If you just pass it in ONE file named FILE, then the output filename is "FILE.tar.bz2" .
## If you give it multiple files, the output is in Archive.tar.gz
## To extract a "mini" archive, use "unmini" .
# Bugs: it ONLY WORKS IN THE CURRENT DIRECTORY! And it probably hates whitespaces.
# maybe this will help with escaping? for f in `ls`; do printf "%q " $f; done
# looks like 'printf' can save the day
function mini() {
    [[ "$#" == 1 ]]         && comp_base="$1" || comp_base="Archive" ## If you pass it one argument, then that argument will also be the filename
    agw_cmd_exists "ionice" && AGW_IONICE="ionice -c3" || AGW_IONICE="" # ionice doesn't exist on BSD/Mac, so check for it
    # xz is 10 times slower than bz2 to compress, but can be ~30% smaller for text.
    # xz is FASTER to decompress than bzip2, and is nearly the same as gzip.
    comp_base=`echo "${comp_base}" | sed "s/[/]//g"` ## Let's remove any slashes from the name of the *output*. That way if someone types "mini MyDir/" it will generate "MyDir.tar.bz2" and NOT "MyDir/.tar.bz2"
    if [[ $(tar --version) =~ .*bsdtar.* ]]; then TAREX=" -c -v "      # BSD tar
    else TAREX=" --preserve-permissions --atime-preserve -c -v "; fi   # GNU tar
    if [[ -f "${comp_base}.tar" || -f "${comp_base}.tar.bz2" ]] ; then
	echo -e "mini: HALTING: Cannot create a new archive---either ${comp_base}.tar or ${comp_base}.tar.bz2 already exists.\nThis is forbidden--you cannot create the archive to overwrite the current archive!!! Delete it and compress again." ;
	return 1;
    else
	if [[ "$#" -gt 1 ]] ; then echo -e "Adding a total of $# files to the archive." ; fi
	for var in "$@"; do echo -e " * Archiving $var --> ${comp_base}.tar.bz2" ; done
	echo -e "[TAR] command running now... maybe should use 'xz' instead of bzip" ;
	eval "$AGW_IONICE" tar "$TAREX" --bzip2 -f "${comp_base}.tar.bz2" $@;
	echo -e "[Done]" ;
    fi
}

## Extracts any number of compressed files of ANY type. Usage: 'unmini a.bz2 b.tar.gz c.tar.bz2 ... etc...'
function unmini() {
    agw_cmd_exists "ionice" && AGW_IONICE="ionice -c3" || AGW_IONICE="" # ionice doesn't exist on BSD/Mac, so check for it
    if [[ $(tar --version) =~ .*bsdtar.* ]]; then TARPARAM=" -x -v "                 # BSD tar
    else TARPARAM=" --preserve-permissions --atime-preserve --keep-old-files -x -v "; fi # GNU tar
    NEVER_OVERWRITE_ZIP=" -n "
    for fff in "$@"; do # Loop through all the input arguments
	echo -e "[:FYI:] * Decompressing \"${fff}\"..."
	case "${fff}" in 
	    *.tar.gz)  EX_CMD="tar ${TARPARAM} --ungzip -f  $fff" ;; 
	    *.tar.bz2) EX_CMD="tar ${TARPARAM}  --bzip2 -f  $fff" ;;
	    *.tar.xz)  EX_CMD="tar ${TARPARAM}     --xz -f  $fff" ;; 
	    *.tar)     EX_CMD="tar ${TARPARAM}          -f  $fff" ;;
	    *.xz)      EX_CMD="xz    --decompress         $fff" ;; 
	    *.gz)      EX_CMD="gzip  --decompress         $fff" ;; 
	    *.bz2)     EX_CMD="bzip2 --decompress         $fff" ;;
	    *.zip)     EX_CMD="unzip ${NEVER_OVERWRITE_ZIP} $fff   -d ${fff/.zip/}" ;;
	    *) echo -e "$ERRTT unmini has failed to detect type of archive (filename is <${fff}>. Recommendation: make sure that file exists)... aborting." ; return 1 ;;
	esac
	eval "${AGW_IONICE}" "${EX_CMD}" # <-- actually decompress the file
    done
}

alias snake="echo -n $'\xF0\x9F\x90\x8D'' '"  # Emoji snake. See http://apps.timwhitlock.info/emoji/tables/unicode
alias cake="echo  -n $'\xF0\x9F\x8D\xB0'' '"  # Emoji cake.  See http://apps.timwhitlock.info/emoji/tables/unicode

function qkiri() { # kill all your OWN jobs
    echo -e "[:FYI:] If qkiri fails, try running this command: qselect -u YOURNAME | sudo xargs qdel -p"
    echo -e "[:HEY:] REALLY terminate LITERALLY EVERY SINGLE ONE OF YOUR JOBS? ***ALL*** OF THEM???"
    select yn in "[:FYI:] Wait a minute! Keep these jobs!" "$ERRTT DELETE THEM WITHOUT REMORSE!!"; do
	case $yn in
	    Wrong*)  echo -e "Ok, not doing anything."; break;;
	    Delete*) bkill 0 -u ${USER}
	esac
    done
    #qselect -u ${USER} | xargs qdel ; echo -e "[Cancelled jobs]"; sleep 1; qstats; break ;;
}

# grep 'pattern' can also be replaced with: perl -nle "print if m{PATTERN}" # <-- beware of '$1' versus "$1" issues!
function qstatlong() { # show qstat with LONG (full) job names:
    qstat -f | egrep -i '(Job Id|Job_Name|Job_Owner|job_state|list.walltime)' | perl -pe 's/(.*Job_Owner.*)@.*/\1/i' | perl -pe 's/.*=\s//' | perl -pe 's/\n/\t/g' | perl -pe 's/(Job Id:\s|$)/\n/ig' | grep -v '^$'
}

function qsass() { # kill all jobs that match this grep... works for usernames, jobs, etc. Be careful!
    # Example usage:  qsass    yourname             # Hopefully yourname is not a subset of someone ELSE's name!
    # or:             qsass    test_job_number_     # Uses GREP, so watch out for invalid regexps!
    #sudo echo 'Initializing sudo permissions here'
    if [[ ${#1} -lt 4 ]]; then
	echo -e "ERR: Your argument to qsass ($1) was not long enough (it is only length ${#1})---it needs to be at least 4 characters, or else we don't believe it's a valid input! Be careful! Even PARTIAL MATCHES will trigger the job deletion. So don't delete 'bad_job' if you don't also want to delete 'not_a_bad_job'."
	return 1;
    fi;
    echo -e "Trying to use qdel to delete jobs that match '$1'...";
    IFS=$'\n' # <-- split a bash string on NEWLINES and not spaces!
    DELETE_US=()
    for jobtext in $(qstatlong | perl -nle "print if m{$1}"); do
	jobid=$(echo -e "$jobtext" | cut -d '.' -f 1)
	echo -e "Found a job ID to delete that matched '$1': $jobid (\"$jobtext\")";
	DELETE_US+=("$jobid")
    done
    echo -e "If you really want to DELETE those jobs, input a '2'."
    select yn in "Wrong! Keep these jobs!" "Delete these jobs from the queue!"; do
	case $yn in
	    Wrong*)  echo -e "Cancelled!"; break;;
	    Delete*) for j in "${DELETE_US[@]}"; do echo -e "Deleting job ID $j..."; `which qdel` $j ; done; break;;
	esac
    done
}

function mvln() { # move and then make an ALIAS in the current location, leaving a "ghostly" alias where the file previously was
    if [[ "$#"  < 2 ]] ; then echo -e "$ERRTT You need to pass AT LEAST ONE filename (source) and ONE DESTINATION (probably a directory?) into this function!"; return 1; fi
    if [[ "$#" != 2 ]] ; then echo -e "$ERRTT Right now, this only works with TWO ARGUMENTS (target and destination)!"; return 1; fi
    SOURCE=$1
    if [[ ! -e "${SOURCE}" ]] ; then echo -e "$ERRTT Your source file (${SOURCE}) does not exist!"; return 1; fi
    DEST=${@: -1}
    if [[ -d "${DEST}" ]]; then # target is a directory
	DEST="${DEST%/}/${SOURCE}" # remove trailing slash. This is OK even with '/' (root), since we always insert a slash at the end no matter what
    fi
    mv "${SOURCE}" "${DEST}"  && ln -s "${DEST}" "${SOURCE}" && echo -e "Moved ${a_green}${SOURCE} ${a_yellow}-> ${a_green}${DEST}${a_end_color} and left a symlink at ${a_cyan}${SOURCE}${a_end_color}"
    if [[ $? != 0 ]]; then
	echo -e "$ERRTT Google Drive (or whatever the filesystem is that is complaining about symlinks) does NOT support cross-filesystem symlinks, so although we did successfully MOVE your file, we were unable to create a symlink. Your file was moved to this location --> ${DEST} "
	return 1;
    fi
}

#function neararchive() { # usage: cd /work/projects/your-project  then: "neararchive myproj-1234-rna-mouse"  --> will move things to the nearline storage and symlink them back
#    if [[ "$#" == 0 ]] ; then echo -e "$ERRTT You need to pass AT LEAST ONE filename into this function!"; return 1; fi
#    D=$(basename `pwd`)
#    TARGET="/nearline_storage/castles_made_of_sand/not_backed_up/${D}"
#    echo -e "We would like to move the following $# files to the NEARLINE SECONDARY STORAGE folder <${TARGET}/>... sound ok to you? Select a NUMERIC option below:"
#    ls -1 -lh $@
#    select yn in "[Cancel]" "Move those files!"; do
#	case $yn in
#	    Move*) mkdir -p "$TARGET"; for f in "$@"; do mv "$f" "${TARGET}/" && ln -s "${TARGET}/$f" ./ ; echo -e "Moved <$f> to <${TARGET}/$f>..." ; done; break;;
#	    *) echo -e "Ok, not doing anything."; break;;
#	esac
#    done
#}

function accuse() { # See who is using the CPU. Usage: just "accuse" with no arguments
    USERS_WITH_JOBS=$(ps aux | tail -n +2 | cut -d ' ' -f 1 | uniq | sort -u)
    echo -e ">> Let's see who is using the CPU..."              >&2 # print to stderr
    echo -e ">> Going to check these users: ${USERS_WITH_JOBS}" >&2 # print to stderr
    TOPTEMP=$(mktemp) # Make a temp file
    TEMP2=$(mktemp)   # Make another temp file
    D=$(date '+%F %H:%M:%S')
    top -b -n 1 | tail -n +7 >> ${TOPTEMP} ;
    echo -e "USERNAME\t%CPU\tBARGRAPH\t# Date = \"${D}\"" >> ${TEMP2}
    NCPU=$(nproc)
    for u in ${USERS_WITH_JOBS}; do \
	cat $TOPTEMP | perl -e "my \$user = ${u}; my \$tot = 0; while(<>){ chomp; my @a=split(/\s+/); if (\$a[2] =~ m/${u}/i) {\$tot += \$a[9];}; } my \$stars = '|' . 'x' x (1+int((\$tot+50)/${NCPU})); print qq{\$user\t\$tot\t\$stars\n};"  >> ${TEMP2}; \
    done ;
    cat ${TEMP2} | column -t -s $'\t'
    /bin/rm ${TOPTEMP} ${TEMP2}
}

function giant() { # find giant files
    #SAVED_IFS=$IFS; IFS=$(echo -en "\n\b")
    echo -e "Calculating all sizes first, then sorting them in ascending order..." >&2 # print to stderr
    DIRS="$@"
    sudo du -sc $DIRS | perl -nle 'print if not m/\btotal\b/' | perl -nle "@a = split(/\s+/); if (\$a[0]>0){print ((\$a[0]/1024.0/1024.0).qq{\t}.\$a[1]);} else { };" | sort -k1,1g | perl -e "my \$t = 0; while(<>) { my @a=split; \$t += \$a[0]; print join(qq{\t}, int(\$t/1024.0).qq{T}, int(\$a[0]).qq{G}, \$a[1]).qq{\n} }"
    #IFS=$SAVED_IFS
}

function giantpleb() { # find giant files
    #SAVED_IFS=$IFS; IFS=$(echo -en "\n\b")
    echo -e "Calculating all sizes first, then sorting them in ascending order..." >&2 # print to stderr
    DIRS="$@"
    du -sc $DIRS | perl -nle 'print if not m/\btotal\b/' | perl -nle "@a = split(/\s+/); if (\$a[0]>0){print ((\$a[0]/1024.0/1024.0).qq{\t}.\$a[1]);} else { };" | sort -k1,1g | perl -e "my \$t = 0; while(<>) { my @a=split; \$t += \$a[0]; print join(qq{\t}, int(\$t/1024.0).qq{T}, int(\$a[0]).qq{G}, \$a[1]).qq{\n} }"
    #IFS=$SAVED_IFS
}

function wholog() {
    echo -e "About to show who logs in and how big their home directories are..." >&2 # print to stderr
    TMP=$(mktemp)
    for f in $(cat /etc/passwd | cut -d ':' -f 1); do echo -e "Checking $f..." >&2 ; sudo lastlog -u $f; done | grep -v '^Username' | perl -pe 's/\s+/\t/' > $TMP
    sudo du -sch /home/* | perl -pe 's/\/home\///' | join.pl -o "NO_HOMEDIR" -1 1 -2 2 $TMP - | column -t -s $'\t'
}

function qing() {
    echo -e "'qing' is 'qstats' but only showing NON-COMPLETED jobs..."
    echo -e "Note that 'qstat -f' may fail in cases where 'qstat' does not. Try plain 'qstat' if this command seems to be taking forever."
    echo -e "'qstat -f' has been known to take upwards of 2 minutes to run."
    qstats | grep -v ' COMPLETE '
    qstat -f -Q
}

function mypip() {
    if [[ "$#" == 0 ]] ; then echo -e "$ERRTT You need to pass AT LEAST ONE PYTHON package-to-install into this function!"; return 1; fi
    echo -e "Installing the following python libraries: $@"
    echo -e "Installing into 'BINFPYROOT' environment variable at: $BINFPYROOT"
    pip2.7 install --ignore-installed --install-option="--prefix=$BINFPYROOT" $@
}

function pylint-agw() {
    if [[ "$#" == 0 ]] ; then echo -e "$ERRTT You need to pass AT LEAST ONE PYTHON filename into this function!"; return 1; fi
    echo -e "Lintifying this file... $FILE (if this fails, run 'pip3 install pylint')"
    DISABLE_THESE_CHECKS="locally-disabled,global-variable-not-assigned,line-too-long,superfluous-parens,bad-whitespace,unused-wildcard-import,trailing-whitespace,unnecessary-pass,missing-docstring,invalid-name,global-statement,multiple-statements,too-many-locals,too-many-statements,too-many-branches,too-few-public-methods,too-many-lines,too-many-instance-attributes,too-many-arguments,wildcard-import,bad-continuation,unidiomatic-typecheck,expression-not-assigned,deprecated-lambda"
    pylint --jobs=4 --output-format=colorized --disable=$DISABLE_THESE_CHECKS "$@"
    # Best usage is: mypylint | s for viewing the colorized output!
}

function mypy-agw() {
    if [[ "$#" == 0 ]] ; then echo -e "$ERRTT You need to pass AT LEAST ONE PYTHON filename into this function!"; return 1; fi
    echo -e "Lintifying this file... $FILE (if this fails, run 'pip3 install mypy')"
    #DISABLE_THESE_CHECKS="locally-disabled,global-variable-not-assigned,line-too-long,superfluous-parens,bad-whitespace,unused-wildcard-import,trailing-whitespace,unnecessary-pass,missing-docstring,invalid-name,global-statement,multiple-statements,too-many-locals,too-many-statements,too-many-branches,too-few-public-methods,too-many-lines,too-many-instance-attributes,too-many-arguments,wildcard-import,bad-continuation,unidiomatic-typecheck,expression-not-assigned,deprecated-lambda"
    mypy "$@"
    # Best usage is: mypylint | s for viewing the colorized output!
}

function mycat() { # cat2 / mycat
    # basically like 'cat' or 'zcat' but fancier
    # Based on file extension, turns something into plain text! Works on mixed files. Only delves one level of compression deep.
    for filename in "$@"; do # Loop through all the input arguments. Doesn't work with redirection.
	VC="cat" # This is the default
	filetype=$(file --dereference --brief "$filename")
	case "$filename" in
	    *.bam)  VC="samtools view -h " ; filetype="BAM_FILE_DETECTED_AGW";;
	esac
	case "$filetype" in 
	    gzip*)     VC="gzip --decompress --stdout " ;; # zcat / gzcat on the Mac
	    bzip2*)   VC="bzip2 --decompress --stdout " ;; # bzcat
	    [xX][zZ]*)   VC="xz --decompress --stdout " ;; # xzcat
	esac
	${VC} "$filename" # <-- actually decompress the file and print it to stdout
    done
}

alias tab="column -t -s $'\t'"

function brutallycompress() {
    # Usage:    brutallycompress  DIRECTORY1   DIRECTORY2   DIRECTORY3
    # Recursively goes through and compresses (with gzip) any files with the extensions listed below.
    if [[ "$#" == 0 ]] ; then echo -e "$ERRTT in arguments to brutallycompress] You need to pass AT LEAST ONE target directory into this function--it can also be '.' for the current directory!"; return 1; fi
    RESTRING=".*[._]\(wig\|bed\|_tracking\|diff\|bed.count\|moa\|fa\|fasta\|fq\|fastq\|txt\|csv\|tab\)"
    echo -e "# About to compress ($COMPRESS)\n#     anything in the following locations: $@\n#     that matches this REGULAR EXPRESSION: $RESTRING"
    echo -e "# -----------------------------------------------------------------------------------------------"
    for location in "$@"; do # Sanity check to make sure all inputs are DIRECTORIES to start
	if [[ ! -d "$location" ]] ; then echo -e "$ERRTT in arguments to brutallycompress]: input '$location' was not a directory. Fix this--only give DIRECTORIES to 'brutallycompress'. Use '.' (not '*') if you want everything in the current directory!"; return 1; fi
    done
    for location in "$@"; do # Loop through all the input arguments. Doesn't work with redirection.
	if [[ ! -d "$location" ]] ; then echo -e "$ERRTT: input '$location' was not a directory!"; return 1; fi
	find "$location" -type f -regex "$RESTRING" -print0 | xargs -0 -I {} sh -c "echo 'Compressing' '{}'; gzip --verbose -6 '{}';" # <-- do **NOT** put double quotes around $COMPRESS (no "")
	#find "$location" -type f \( -iname "*.bed" -o -iname "*_tracking" -o -iname "*.diff" -o -iname "*.bed.count" -o -iname "*.moa" -o -iname "*.fa" -o -iname "*.fq" -o -iname "*.fasta" -o -iname "*.fastq" -o -iname "*.txt" \) -print0 | xargs -0 $COMPRESS
    done #  xargs -I %%% sh -c 'echo %%%; echo %%%;'
}

function rtag() {
    # Recursively set finder tags
    tag=$1
    shift
    for fff in "$@"; do # Loop through all the input arguments. Doesn't work with redirection.
	#find . -exec tag --add tagname {} \;  -print
	#find $name -type f | xargs xattr -wx com.apple.FinderInfo "`xattr -px sampleFile`"
	echo -e "Adding tag $tag to $fff"
    done
}

function wgetdevour() { # Downloads certain types of files via wget for a password-protected directory
    if [[ "$#" != "3" ]] ; then echo -e "$ERRTT You need to pass THREE arguments -- username, password, directory!"; return 1; fi
    USERNAME=$1
    PASSWORD=$2
    DIRECTORY=$3
    wget -mk -r -U mozilla --connect-timeout=90 --random-wait -e robots=off --limit-rate=1m --tries=99 --timeout=50 \
	 -A md5,fasta,fa,fastq,fq,bz2,gz,Z,gzip,bzip2,pdf,xls,xlsx,bed,sam,bam,vcf,csv,txt,sh,complete \
	 --no-parent \
	 --http-user="$USERNAME" --http-passwd="$PASSWORD" "$DIRECTORY/" # trailing slash here is crucial
}


function hist() {
    history | tail -n 100
}

QUEUE_LOGFILE_PATTERN='.*.\([eo][0-9]+\|.log.err\|.log.out\|.out.log\|.err.log\)'
# ~~~~~~~~~~~~~~~ QUEUE / QSUB / BSUB LOG VIEWING AND MOVING ~~~~~~~~~~~~~~~
function rmlogs() { # Usage:     catlogs (no arguments)
    #if [[ "$#" == "0" ]] ; then SEARCH_DIRS=. ; else SEARCH_DIRS=$@ ; fi
    NUM_FOUND=$(find . -maxdepth 1  -type f  -regex "${QUEUE_LOGFILE_PATTERN}" | wc -l)
    echo -e "[:OK :] Deleting $NUM_FOUND 'qsub' logs (the files ending with .e1234 or .o456, etc. or .log.err or .log.out)..."
    find . -maxdepth 1 -type f -regex "${QUEUE_LOGFILE_PATTERN}" -delete
    echo -e "[:OK :] Deleted the ${NUM_FOUND} log files.."
}
function catlogs() { # Usage:     catlogs (no arguments)
    if [[ "$#" == "0" ]] ; then SEARCH_DIRS=. ; else SEARCH_DIRS=$@ ; fi
    echo -e "Concatenating all QSUB/BSUB logs in this directory and viewing it in 'less'..."
    find ${SEARCH_DIRS} -maxdepth 1 -type f -regex "${QUEUE_LOGFILE_PATTERN}" -print0 | xargs -0 -I{} paste.pl "FILE={}" {} | s
}
function mvlogs() { # Usage:   mvlogs OUTPUT_DIRECTORY
    # This function moves those annoying qzub logs to the destination directory
    if [[ "$#" != "1" ]] ; then echo -e "$ERRTT mvlogs needs EXACTLY ONE destination directory!"; return 1; fi
    if [[ ! -d $1 ]]     ; then echo -e "$ERRTT mvlogs needs an EXISTING DIRECTORY as a destination! The specified directory ($1) appears not to exist."; return 1; fi
    find . -maxdepth 1 -type f -regex "${QUEUE_LOGFILE_PATTERN}" -print0 | xargs -0 -I{} mv {} $1/
    echo -e "[:OK :] Moved all the logs into the directory <$1>"
}
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

function star_make_index() {
    # This makes an alignment directory for the STAR aligner, when given just a FASTA file and a length.
    # The exact length is not critical, but it should ideally be the same length as the reads, if possible (slightly longer is OK too, but not necessary).
    if [[ "$#" != "2" ]] ; then echo -e "$ERRTT star_make_index needs an input FASTA file and a 'how long are the reads' parameter!"; return 1; fi
    FAS=$1
    LEN=$2
    CPU_THREADS=8
    if ! [[ ${FAS^^} =~ \.*[.](FASTA|FA)$ ]]; then echo -e "Arg1 must be .fasta or .fa file, but it was: $FAS"; return 1; fi
    if ! [[ ${LEN^^} =~ ^[0-9]+$ ]]; then echo -e "Arg2 must be a (numeric integer) read length, but it was: $LEN"; return 1; fi
    OUT=$(basename $FAS | perl -pe 's/[.](fasta|fa)$//i')
    OUT=$(echo `pwd`/star_output.$OUT)
    (>&2 echo -e "Output file is:  " $OUT)
    (>&2 echo -e "About to generate a STAR genome index. Settings are as follows:")
    (>&2 echo -e "        FASTA: $FAS")
    (>&2 echo -e "       LENGTH: $LEN")
    (>&2 echo -e "   OUTPUT DIR: $OUT")
    (>&2 echo -e "       N CPUS: $CPU_THREADS")
    cmd="mkdir -p $OUT && STAR --runMode genomeGenerate --genomeDir $OUT --genomeFastaFiles $FAS --runThreadN $CPU_THREADS"
    (>&2 echo -e "Here is the command you should now run:\n   $cmd\n   (Copy and paste that command above, or run it with 'qplz' and double quotes: qplz -t $CPU_THREADS -m 8 \"command\""  )
}

function convert_chromosome_to_chr1_for_genome_browser() {
    if [[ "$#" != "2" ]] ; then echo -e "$ERRTT needs two inputs: 1. a chromosome to turn into 'chr1' and 2. a bam filename to convert"; return 1; fi
    FINDCHR=$1
    IN=$2
    NEWCHR=chr1
    TMPSAM=${IN}--TMP.sam
    if [[ ! -e $IN ]] ; then echo -e "$ERRTT Your specified input file ($FILE) does not exist!"; return 1; fi

    FINALBAM=${IN/.bam/--${NEWCHR}_only.bam}

    (>&2 echo -e "[CONVERSION UPDATE] Fixing the @SQ lines at the top of the file...")
    samtools view -H $IN | perl -ne "print if (not m/^[@]SQ/ or s/^([@]SQ.*SN:)$FINDCHR(\s)/\$1$NEWCHR\$2/)" >| $TMPSAM
    samtools view    $IN \
	| awk "\$3==\"$FINDCHR\" {print;}" \
	| awk -v OFS='\t' "{\$3=\"$NEWCHR\";print;}" \
	| cat $TMPSAM - \
	| samtools view -b -S - \
	>| $FINALBAM
    /bin/rm $TMPSAM

    (>&2 echo -e "<$FINALBAM> generated! Chromosome changed from '$FINDCHR' to 'chr1'. Original input: <$IN>")
}

function convert_pdf_to_png() {
    for f in "$@" ; do echo -e "Converting $f..."; convert -verbose -density 72 $f +antialias ${f/.pdf}.png; done
}

function convert_webp_to_jpg() {
    # This will be a lossy conversion. Only useful if we can't use webp.
    # Converts ALL `.webp` files in a directory. Beware
    JPEG_QUALITY=65
    for IN in *.webp; do
        OUT=${IN/.webp/.jpg}
        echo "${IN} --> ${OUT}";
        jpeg_tempfile=$(mktemp)
        dwebp "${IN}" -o "${jpeg_tempfile}"  # Requires that `dwebp` is installed.
        convert "${jpeg_tempfile}" -quality $JPEG_QUALITY "${OUT}"
        touch -r "${IN}" "${OUT}"  # Copy certain attributes (like file modification time)
        \rm ${jpeg_tempfile}       # Delete the temp file
    done
    # Note that this keeps the .webp images.
    #find ./ -name "*.webp" -exec dwebp {} -o {}.jpg \;
}

function zwcl() {
    zgrep -c '' "$@"
    #for f in "$@" ; do echo -e -n "$f\t"; zgrep -c '' $f; done # don't use zless, it's super slow compared to zcat
}

function pybug() {
    #echo -e "$1"       # First argument
    #echo -e "2 ${@:2}" # Arguments 2 and beyond
    if [[ ! -e "$1" ]]; then
	echo -e "Looks like your (required!) python script <$1> could not be found!"; return 1;
    fi
    echo -e "Running [jupyter]..."
    jupyter --pdb "$1"     --      "${@:2}"
    #             SCRIPT  DASHES   ARGUMENTS
}

function jupynote() {
    # Executes a jupyter notebook with generous settings. Saves output HTML to desktop
    if [[ "$#" != "1" ]] ; then echo -e "$ERRTT 'jupynote' can only accept exactly one argument: the name of a .ipynb file to run and convert to html"; return 1; fi
    #                                                                                   14400 is 4 hours
    jupyter nbconvert --output-dir=~/Desktop/ --to 'html' --ExecutePreprocessor.timeout=14400 --execute "$1"
    
    EXIT_CODE=$?
    if [[ "$EXIT_CODE" == "0" ]]; then
	echo "[:OK:] Done executing $1. Successful exit code $EXIT_CODE."
    else
	echo "$ERRTT Done executing $1. Non-zero exit code $EXIT_CODE"
    fi
}

#function docker-begone {
#    echo -e "Stopping and removing ALL DOCKER CONTAINERS. ALL OF THEM!"
#    docker stop $(docker ps -a -q)
#    docker rm $(docker ps -a -q)
#    echo -e "Not even one docker container was spared."
#    echo -e "Clearing out space also..."
#    docker rm $(docker ps -q -f 'status=exited')
#    echo -e "You may also have to, with RUNNING YOUR CONTAINERS, run this: docker system prune -a"
#    echo -e "The file that should be small now is:"
#    ls -lah $HOME/Library/Containers/com.docker.docker/Data/com.docker.driver.*/*.qcow2
#    echo -e "Clearing out space for any 'dangling' docker items (probably there are none)..."
#    docker rmi $(docker images -q -f "dangling=true")
#}

# count bases per line in a fasta file:
# cat x.fa | grep -v '^>' | perl -ne 'chomp; my @a=split(//,$_); my %c=(); for my $x (@a) { $c{lc($x)}++; }; for my $k (keys(%c)) { print($k.qq{=}.$c{$k}."\t"); }; print qq{  <-- \n};'

function hsync() {
    echo -e "[:FYI:] HTML syncing..."
    rsync -avz $HOME/workspace/html_master/ $HOME/web/
}

function fasta2single_line_per_seq() {
    echo -e "[:HEY:] FYI, this script reads from STDIN, and inputs to STDOUT. We'll be waiting for a cat-ed in input FASTA file." >&2 # print to stderr
    if [[ "$#" != 0 ]] ; then echo -e "$ERRTT This script takes NO ARGUMENTS. You must 'cat' a FASTA file to pass it in, like this:  cat myfile.fa | fasta2single_line_per_seq | gzip > done.fa.gz  "; return 1; fi
    awk '!/^>/{printf "%s", $0; n="\n"} /^>/{print n $0; n=""} END {printf"%s",n}'
}

function j1() {
    if ! agw_cmd_exists "bsub"; then
	echo -e "$ERRTT This is NOT a machine on the cluster with the BSUB queue. Connect to the cluster machine first!"; return 1;
    fi
    : "${cpre}=\033}" ## <-- Color prefix, if not already set
    : "${a_end_color:=${cpre}[m}"
    : "${a_ok_color:="$(tput setaf 2)"}"  # Green, if not already set
    : "${a_err_color:="$(tput setaf 1)"}" # Red, if not already set
    : "${a_orange136}:=${cpre}[38;5;136m}"
    : "${a_yellow226}:=${cpre}[38;5;226m}"
    : "${a_blue_bold}:=${cpre}[1;34m}"
    : "${a_red_bold}:=${cpre}[1;31m}"
    ARRAY_JOB_TEXT=$(bjobs -A -u "${USER}" 2>&1) # Captures STDERR+STDOUT
    if [[ "${ARRAY_JOB_TEXT}" =~ .*array\ is\ not\ found.* ]]; then
	echo -e "${a_blue_bold}(No active array jobs.)${a_end_color}"
    else
	echo -e "${a_blue_bold}Array job status (-A):${a_end_color}"
	echo -e "${ARRAY_JOB_TEXT}"
	bjobs -w -A -u "${USER}" # wide to show FULL array names
	echo -e "[:FYI:] To cancel ALL jobs in an array --> bkill -J \"JOBNAME*\" <-- be careful!"
    fi
    echo -e "${a_blue_bold}Current & finished-within-the-last-hour jobs:${a_end_color}"      # memlimit:3 run_time:6 time_left:5 job_group:10 from_host:12 
    JOB_TEXT=$(bjobs -a -o "jobid:8 stat:4 job_name:80 user:4 queue:3 cpu_used:6 max_mem:3 exec_host:18 submit_time:12 finish_time:12 delimiter='|'" 2>&1)
    JOB_TEXT=$(echo -e "${JOB_TEXT}" | perl -pe "s/^(.*[|]RUN.*)\$/${a_ok_color}\$1${a_end_color}/g")
    JOB_TEXT=$(echo -e "${JOB_TEXT}" | perl -pe "s/^(.*[|]DONE.*)\$/${a_orange136}\$1${a_end_color}/g")
    JOB_TEXT=$(echo -e "${JOB_TEXT}" | perl -pe "s/^(.*[|]EXIT.*)\$/${a_err_color}\$1${a_end_color}/g")
    JOB_TEXT=$(echo -e "${JOB_TEXT}" | perl -pe "s/^(.*[|]PEN.*)\$/${a_yellow226}\$1${a_end_color}/g") # pending
    echo -e "${JOB_TEXT}"
    FAILED_JOB_TEXT=$(bjobs -a -x -l  2>&1)    # Capture STDERR and STDOUT both to a variable returns 0 no matter what. If there's no job, it returns the verbatim text "No job found"
    if [[ "${FAILED_JOB_TEXT}" =~ No.*job.* ]]; then
	echo -e "${a_blue_bold}(No recent job failures were found.)${a_end_color}"
    else
	echo -e "$ERRTT: ${a_red_bold}Jobs that failed (-a -x -l output):${a_end_color}"
	echo -e "${FAILED_JOB_TEXT}"
	echo -e "$ERRTT: [End of list]"
    fi
}

function jj() {
    j1 | less -S --RAW-CONTROL-CHARS -f --IGNORE-CASE
}

function setsubtract() {
    # input files do NOT need to be sorted
    if [[ "$#" != 2 ]] ; then echo -e "$ERRTT You need to pass EXACTLY TWO files to the set subtraction!"; return 1; fi
    if [[ ! -f "$1" ]] ; then echo -e "$ERRTT Could not find the specified input file: $1!"; return 1; fi
    if [[ ! -f "$2" ]] ; then echo -e "$ERRTT Could not find the specified input file: $2!"; return 1; fi
    grep -F -x -v -f "$2" "$1"
}

function setunion() {
    # input files do NOT need to be sorted
    if [[ "$#" != 2 ]] ; then echo -e "$ERRTT You need to pass EXACTLY TWO files to the set union!"; return 1; fi
    if [[ ! -f "$1" ]] ; then echo -e "$ERRTT Could not find the specified input file: $1!"; return 1; fi
    if [[ ! -f "$2" ]] ; then echo -e "$ERRTT Could not find the specified input file: $2!"; return 1; fi
    cat "$1" "$2" | sort -u
}


#function checkbackup {
#    # checks the MOST RECENT file in /it/backup/log, and gives you the last 10 lines from that file
#    echo -e ""
#    echo -e "Reporting the last 10 lines of the most recent backup on this machine."
#    echo -e "If these lines are not from yesterday or today's date, then something is seriously wrong!"
#    tail -n 10 /it/backup/log/$(ls -1t /it/backup/log/ | head -n 1)
#    TODAY=$(date +"%Y%m%d")
#    YESTERDAY=$(date +"%Y%m%d" --date='1 day ago')
#    LAST_BACKUP_DATE=$(ls -1t /it/backup/log/ | head -n 1 | cut -f 1 -d '-')
#    [[ ("$LAST_BACKUP_DATE" == "$TODAY") || ("$LAST_BACKUP_DATE" == "$YESTERDAY") ]]; RESULT=$?
#    if [[ $RESULT -ne 0 ]] ; then echo -e "***\n**\n*\n******************** WARNING --- last backup date was on $LAST_BACKUP_DATE ************\n*\n**\n***";
#    else echo -e "\nOK: The last backup ($LAST_BACKUP_DATE) was made within the last 2 days.\n"; fi
#}


#function workarchive { # usage: cd /work/projects  then: "workarchive myproj-1234-rna-mouse"  --> will move things to bag_of_holding/work_archive/
#    D=${1%/}
#    DARCH=${D}_ARCHIVE
#    TARGET=/bag_of_holding/work_archive/${D}
#    agw_cmd_exists "ionice" && AGW_IONICE="ionice -c3" || AGW_IONICE="" # ionice doesn't exist on BSD/Mac, so check for it
#    echo -e "We would like to move the directory (without a slash) \"${D}\" to the final TAPE DRIVE target \"${TARGET}\"... sound ok to you? Select a NUMERIC option below:"
#    select yn in "[Cancel]" "Move those files!"; do
#	case $yn in
#	    Move*) sudo mv ./${D} ./${DARCH}
#		 ln -s ${TARGET} ./${D}
#		 eval sudo ${AGW_IONICE} mv ./${DARCH}  ${TARGET}
#		 break;;
#	    *) echo -e "Ok, not doing anything."; break;;
#	esac
#    done
#}

#function truename {
#    qsub -I  -l ncpus=1  -l mem=4gb  -l walltime=24:00:00 -N "INTERACTIVE_for_$USER" -q Interactive -W group_list="interactive"
#}
#
#function qsud {
#    if [[ "$#" == "0" ]] ; then echo -e "$ERRTT qq (queue submission) needs at least ONE argument script!"; return 1; fi
#    PBS_BIO_GGG="Bio"
#    PBS_BIO_QQQ="bioqueue"
#    PBS_GEN_GGG="General"
#    PBS_GEN_QQQ="genqueue"
#    groupstr=`groups`
#    case "$groupstr" in 
#	*bioqueue*)
#    	    QGRP="$PBS_BIO_GGG"
#	    QQUE="$PBS_BIO_QQQ"
#	    ;;
#	*genqueue*)
#    	    QGRP="$PBS_GEN_GGG"
#	    QQUE="$PBS_GEN_QQQ"
#	    ;;
#	*)
#    	    echo -e "$ERRTT your username was not in a relevant group -- it must be in either the 'bioqueue' or 'genqueue' groups. This is MANDATORY if qsud is to work!"
#	    return 99
#	    ;;
#    esac
#    cmd=( "qsub" "-q" $QGRP "-V" "-l walltime=320:59:59" "-W group_list=\"$QQUE\"" "$@" ) # 14 days is the longest possible
#    echo -e "About to execute this command. Note that all paths must be FULL PATHS:\n----------\nCOMMAND>      ${cmd[@]}\n----------"
#    "${cmd[@]}" # <- Actually execute. See http://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters
#}


function bsubnow() {
    if (( "$#" >= 1 )); then
	PROJNAME=$1;
    else
	PROJNAME='YOUR_PROJNAME_GOES_HERE'
    fi
    if (( "$#" >= 2 )); then
	CMD=$2;
    else
	CMD='YOUR_COMMAND_GOES_HERE'
    fi
    echo bsub -L '/bin/bash' \
	 -R "'span[hosts=1]'" \
	 -q "'medium'" \
	 -R "'rusage[mem=32]'" \
	 -oo "'LOG-%J.bsub.log.out'" \
	 -eo "'LOG-%J.bsub.log.err'" \
	 -n 1 \
	 -J "'${PROJNAME}'" \
	 \"${CMD}\"
}

function bsubtemp() {
    if (( "$#" < 1 )) ; then echo -e "$ERRTT Please specify a job name! Note that you probably also want to send this output to STDOUT, so e.g. 'bsubtemp MYJOB > job.sub.sh'"; return 1; fi
    if [[ "$#" == 2 ]] ; then
	MEM=$2
    else
	MEM=4  # gigabytes
    fi
    if ! agw_is_numeric "${MEM}"; then
	echo "$ERRTT Your second argument, if included, MUST be an integer (the amount of RAM to request, in GB)! Your argument was this: ${mem}" >&2;
	return 1;
    fi
    JJJ=${1/ /_} # spaces --> '_'
    echo -e "#!/bin/bash"
    echo -e "set -euo pipefail; shopt -s nullglob; shopt -s globstar;"
    echo -e "#BSUB -L /bin/bash                # Ensure that we are using BASH"
    echo -e "#BSUB -R 'span[hosts=1]'          # No splitting across hosts"
    echo -e "#BSUB -q medium                   # short=2 hours, medium=24 hours"
    echo -e "#BSUB -R 'rusage[mem=${MEM}]'     # In GB"
    echo -e "#BSUB -n 1                        # N threads"
    echo -e "#BSUB -J ${JJJ}"
    echo -e "#BSUB -o ${JJJ}-%J-%I.bsub.log.out   # or '-oo' to OVERWRITE (instead of appending)"
    echo -e "#BSUB -e ${JJJ}-%J-%I.bsub.log.err   # or '-eo' to OVERWRITE (instead of appending)"
    echo -e ''
    echo -e ": \"\${LSB_JOBID:=}\" # a non-defined ID would become blank"
    echo -e "if [[ -z \"\${LSB_JOBID}\" ]]; then"
    echo -e "    echo \"$ERRTT ERROR: You should NOT run this as a shell script! Correct way -->   bsub < scriptname.sh      <-- note: the '<' is important! Do not omit it.\"; "
    echo -e "    exit 1; "
    echo -e "fi"
    echo ''
    # 'bhist' has more info too: # bhist -w -l -a -n 99 769084 | less -S
}

function agw_is_numeric() {
    numeric_regex='^[0-9]+$'
    if [[ "$1" =~ ${numeric_regex} ]]; then
	return 0 # IS a number (0 is 'good' in bash)
    else
	return 1 # NOT a number
    fi
}


# Logs you into a fast interactive node
alias qrush='bsub -q long   -n 4   -R '\''span[hosts=1] rusage[mem=4]'\'' -Is bash' # was previously 'qrsh' 

function windowserver_kill() {
    sudo killall -HUP WindowServer
}


function bbq() { # bsub, be quick!
    # Usage:   bbq "my command here"
    # Note:    * Outputs log files to THE CURRENT DIRECTORY!
    #          * Makes a temp file using 'mktemp'
    #          * Blocks until the submitted job is complete (with bsub -K).
    #          * It is NOT OK to cancel out of the submitted job! It will cancel the job!
    : "${cpre}=\033}" ## <-- Color prefix, if not already set
    : "${a_end_color}:=${cpre}[m}"
    : "${a_blue_bold}:=${cpre}[1;34m}"
    : "${a_green}:=${cpre}[32m}"
    echo -e "${a_blue_bold}BSS${a_end_color}: ${a_blue_bold}B${a_end_color}sub ${a_blue_bold}B${a_end_color}e ${a_blue_bold}Q${a_end_color}uick!"
    if [[ "$#" < 1 || "$#" > 2 ]] ; then echo -e "\n$ERRTT BBQ: You must specify EXACTLY ONE command to add to the queue run---put your command in quotes! Example:  bbq \"ls -al\"  "; return;  fi
    cmd=$1
    
    if (( "$#" >= 2 )); then
	mem=$2
	if ! agw_is_numeric "${mem}"; then
	    echo "$ERRTT Your second argument, if included, MUST be an integer (the amount of RAM to request, in GB)! Your argument was this: ${mem}" >&2;
	    return 1;
	fi
    else
	mem=4 # GB
    fi
    ttt=$(mktemp --suffix=".${USER}.bsub.sh")
    jobname="Q_${USER}_$(date +%s)"
    echo       -e "(Remember that backslashes in your command may need to be doubled.)"
    echo       -e "(Remember that DOLLAR SIGNS MUST BE ESCAPED! So use \\$ instead of '$'.)"
    echo       -e "Submitting this (temporary) script: ${a_blue_bold}${ttt}${a_end_color}"
    echo       -e "Command to run:"
    printf "%s\n" "${cmd}"
    echo       -e ""
    bsubtemp "${jobname}" ${mem}    >| ${ttt}
    printf "%s\n" ""                >> ${ttt}
    printf "%s\n" "${cmd}"          >> ${ttt}
    printf "%s\n" ""                >> ${ttt}
    #bsub -K   -q 'short' < ${ttt}
    echo -e "Running the job here locally... blocking until it finishes!"
    echo -e "(You probably want to be running this job in SCREEN or TMUX.)"
    echo -e "You can check the BSUB command with -->  ${a_blue_bold}less -S ${ttt}${a_end_color}"
    echo -e "${a_green}[WAITING FOR JOB \"${jobname}\"... DO NOT CTRL-C THIS JOB!]${a_end_color}"
    echo -e "${a_green}[WARNING: IF YOU CTRL-C, THE QUEUE JOB IS ABORTED, EVEN IF IT IS RUNNING!]${a_end_color}"
    echo -e "${a_green}View the output LIVE with: tail -f PROJECT_ID_HERE.bsub.log.err <-- only works for ONE file at a time${a_end_color}"
    bsub -K < ${ttt}
    echo -e "${a_green}[DONE]${a_end_color}"
}

function convert_webp_to_jpg() {
    # Requires 'dwebp'
    # Batch convert recursively any WEBP format to JPEG. Keeps old jpegs.
    find ./ -name "*.webp" -exec dwebp {} -o {}.jpg \;
}

function mp4_concat() {
    # Concatenate 2+ split-up mp4 files into a single file. Requires ffmpeg
    if [[ "$#" < 2 ]]; then
	echo "Requires at least 2 files."
	return
    fi
    INFILE=file_combiner_temp.tmp
    # Note that infile has to be in the same directory as the mp4s if you
    # are not going to include the full path (which this command does not)
    
    OUTFILE=OUTFILE.mp4
    if [[ -e "$OUTFILE" ]]; then
	echo "$OUTFILE already exists--not overwriting."
	return
    fi
    if [[ -e "$INFILE" ]]; then
	echo "$INFILE temp file already exists--not overwriting."
	return
    fi
    for arg; do
	printf 'Preparing to combine "%s"...\n' "$arg"
	touch $INFILE
	echo "file '$arg'" >> $INFILE
    done
    echo "Writing to output file $OUTFILE..."
    ffmpeg -f concat -safe 0 -i "$INFILE" -c copy "$OUTFILE"
}

export ANDROID_HOME=~/.android

function delete_android_file_transfer_on_mac() {
    PID=$(ps -fe | grep "[A]ndroid File Transfer Agent" | awk '{print $2}')
    if [[ -n ${PID} ]]; then kill ${PID}; fi;
    mv "/Applications/Android File Transfer.app/Contents/Resources/Android File Transfer Agent.app" \
       "/Applications/Android File Transfer.app/Contents/Resources/Android File Transfer Agent DISABLED.app"
    mv "${HOME}/Library/Application Support/Google/Android File Transfer/Android File Transfer Agent.app" \
       "${HOME}/Library/Application Support/Google/Android File Transfer/Android File Transfer Agent DISABLED.app"
    osascript -e 'tell application "System Events" to delete every login item whose name is "Android File Transfer Agent"'
    echo "Done disabling it. Note that the files have been RENAMED, and not totally deleted!"
}

function gitstory() {
    if [[ "$#" != 1 ]] ; then
	echo -e "$ERRTT You must specify EXACTLY ONE filename."; return;
    fi
    F=$1
    if [[ ! -f $F ]]; then
	echo -e "$ERRTT Apparently your specified file was not found??? Check this file: $F"; return;
    fi

    echo "[:OK:] Checking the last few checked-into-git versions of <$F>..."
    filebase=$(basename -- "$F")
    extension="${filebase##*.}"
    filepre="${filebase%.*}"
    git show HEAD~1:$F > $filebase.01.git.old_from_git_1_rev_back.01.${extension}
    git show HEAD~2:$F > $filebase.02.git.older_2_revisions__back.02.${extension}
    git show HEAD~4:$F > $filebase.04.git.older_4_revisions__back.04.${extension}
    git show HEAD~8:$F > $filebase.08.git.older_8_revisions__back.08.${extension}
    echo "[:OK:] Generated a bunch of '.git.old' files"
    echo "[:FYI:] If you're looking for something really specific, remember you can use:  git log -p FILENAME | less -S"
}

PG_DB="/usr/local/var/postgres"
alias pg_start="pg_ctl -D ${PG_DB} start"
alias pg_stop="pg_ctl  -D ${PG_DB} stop"

function vv {
    # Running the 'vd' table viewer while FORCING a tab delimiter
    vd --filetype='tsv' --delimiter='${TAB}' "$@"
}

function cheat() {
    VERBATIM_CHEAT_SHEET=$(cat <<'END_OF_HEREDOC'

>> Fix an iPhone syncing forever in the Finder: This should solve the <<Finder can't quit because an operation is still in progress on an iOS device>> problem: killall -9 AMPDevicesAgent

>> UNIX / APT / APT-GET: Look for total size of installed packages in KB (so divide by 1000 for MB) dpkg-query -Wf '${Installed-Size}\t${Package}\n' | sort -n | tail -n 20

>> VSCode example launch.json [// means 'newline']:
   { "configurations": [  {
            "name": "MyProgram",
            "type": "python",
            "request": "launch",
            "program": "path/to/your/script.py",
            "args": [ "--good-arg", "some-value" ],
            "console": "integratedTerminal"          } ], }

>> MAC: Put the Mac application switcher ("Alt tab") on ALL (multiple) monitors: 1. Run this command ---> defaults write com.apple.Dock appswitcher-all-displays -bool true  and then 'killall Dock' (or reboot your Mac).
>> MAC Time machine: SYSTEMWIDE (not user-specified) exclusions are in: /System/Library/CoreServices/backupd.bundle/Contents/Resources/StdExclusions.plist
>> MAC Time machine: USER SPECIFIED exclusions: view them with plutil -convert xml1 -o ~/Desktop/out_temp.txt /Library/Preferences/com.apple.TimeMachine.plist && cat ~/Desktop_out_temp.txt.   Convert that file BACK with plutil -convert binary1 ~/Desktop/out_temp.txt -o ~/Desktop/out_temp_converted.xml (save this to the com.apple.TimeMachine.plist). Note that this is GLOBAL.
>> MAC Time Machine: exclude/check: tmutil isexcluded/tmutil addexclusion (-p) PATH  
>> MAC Time Machine: delete backups: tmutil delete /Volumes/DISK/Backups.backupdb/COMPNAME/  

>> Rscript/R read STDIN: One-liner R and perl extravaganza for plotting contig lengths:  cat contigs.fasta  | perl -ne 'next if not m/^>/; chomp; m/>NODE_(\d+).*length_([\d.]+).*cov_([\d.]+)/; print join(qq{\t}, $1, $2, $3).qq{\n};' | Rscript --vanilla -e 'ddd=read.table(file("stdin"),header=F,row.names=1); pdf("a.pdf"); plot(ddd[,1], ddd[,2]); dev.off();'  
>> UNICODE get character as shell escape: printf YOURCHAR | hexdump , then convert that like this: printf "\xE2\x98\xA0"  
>> Amazon AWS / S3: upgrade awscli on an instance:  pip install awscli --upgrade --user    
>> DOCKER: list even EXITED containers: docker ps -a  
>> DOCKER: connect interactively no matter what: docker run --name prokka -v '/:/MacRoot/' -i -t --entrypoint '/bin/bash' YOURIMAGE  
>> DOCKER: start a container from an image 1: docker run --name NEWNAME --rm -i -t IMGNAME bash  
>> DOCKER: start a container from an image 2: docker run -it 0134abd1356  <-- check 'docker images' for a list of images  
>> DOCKER: attach to container: docker exec -it CONTAINERNAME bash  
>> EMOJI TO TERMINAL: example: a snake is echo $'\xF0\x9F\x90\x8D' (See http://apps.timwhitlock.info/emoji/tables/unicode)  
>> SAFARI: Disable page previews: defaults write com.apple.Safari DebugSnapshotsUpdatePolicy -int 2  
>> ZIP: Unzip files into SUBDIRECTORIES always: ls *.zip|awk -F'.zip' '{print "unzip "$0" -d "$1}'|sh   
>> ZIP: Zip on Mac without dotfiles (dot files): zip -r -X out.zip input_directory_here    -x "*.DS_Store"  
>> RAM/Filesystem: clear out disk cache, recover RAM: purge  
>> MAC: what program is using a disk / preventing eject: sudo lsof -xf +d /Volumes/THEVOL  -- or try diskutil unmount /Volumes/THEVOL  to get exactly which processes are preventing eject / unmount
>> MAC: lock a file, for example to prevent Dropbox from replacing a custom icon: chflags uchg ~/Dropbox/Icon$'\r'  (use nouchg to unlock a file) 
>> MAC disk: specific file I/O: fs_usage <programname>  
>> MAC disk: filesystem I/O:    fs_usage -w | grep -v '0\.0'   
>> MAC set startup disk: sudo bless -mount /Volumes/YOURDISK -setBoot  
>> MAC: show weird file flags: ls -l -@  
>> MAC: rebuild spotlight for the boot drive only: sudo mdutil -E /  
>> MAC: recursively clear worthless quarantine flags: xattr -r -d com.apple.quarantine FILENAME  
>> MAC: delete impossible-to-delete files find FILE -flags schg -exec chflags noschg {} \;  
>> MAC: show library: chflags nohidden ~/Library/  
>> MAC: batch change modification time: find . -print0 | xargs -0 SetFile -d '12/31/2012 12:00:00 PM'  
>> MAC: No quarantine warn  : defaults write com.apple.LaunchServices LSQuarantine -bool NO  
>> MAC: check code signing: codesign -vvv /Applications/iTunes.app  
>> MAC: what is connected over ethernet locally, by Internet Sharing? arp -i en1 -a  
>> MAC: No .DS_Store servers: defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool YES  
>> MAC: Select in QuickLook (buggy!): defaults write com.apple.finder QLEnableTextSelection -bool YES  
>> MAC: No smooth scroll: defaults write -g NSScrollAnimationEnabled -bool NO  
>> BASH: Print and execute a command: use a SUBSHELL for this:  (set -x; YOUR COMMAND THAT IS PRINTED; ); next command not printed  
>> FIND: delete files when argument list too long: find . -iname '*whatever*' -delete  
>> CRON: edit crontab cron.tab with emacs: env EDITOR=emacs crontab -e  
>> CRON: check CRON logs (if enabled) on the Mac: sudo cat /var/mail/your_username_here  
>> SUDO Delayed sudo: sudo sh -c "(sleep 3600; do your command here )"  
>> IMAGE: Display image in ASCII on the command line: 'tiv' IMAGENAME  
>> IMAGE SIPS: PNG -> JPEG: mkdir outjpegs; sips -s format jpeg *.png --out outjpegs  
>> IMAGE Imagemagick SVG -> PNG: convert -format png -background transparent -density 300 in.svg out.png (see also the -resize 100 option) 
>> IMAGE Imagemagick convert multi-page pdf: for a in $(seq 90); do echo "Converting page $a..."; convert -quality 80 your.pdf[$a] OUT_page_$a.jpg ; done   (note: REQUIRES ghostscript (brew install gs) or it will crash)  
>> IMAGE Imagemagick convert recursive: find ./ -name "*.jp*" -print0 | xargs -0 -I{} convert -resize 1024x768 -quality 85 {} {}  # or resize 50% 
>> IMAGE Imagemagick convert all PDFs in a dir: for f in *.pdf ; do echo "Converting $f..."; convert -verbose -density 144 $f +antialias ${f/.pdf}.png; done  
>> IMAGE Imagemagick convert PDF -> PNG w/antialias and downsample: convert -verbose -density 144 in.pdf +antialias out.png   (note: REQUIRES ghostscript (brew install gs) or it will crash)  Note that argument order is very important and may not be correct here. It is also very slow.. 
>> MP3: Convert m4a/ogg->mp3 for name in *.ogg; do ffmpeg -i "$name" -ab 256k -map_metadata 0:s:0 "Converted_${name/.ogg/.mp3}"; done;  
  
>> UNIX: Check directories for differences (by modification time): ionice -c3 rsync --dry-run --delete -vr /dirA /dirB  
>> FIND: Find all .R files in a directory:   find ./ -name "*.R" -exec ls \'{}\' \;  
>> DIFF: directories:   diff -rq DIR1 DIR2  
>> DIFF: with side-by-side and line numbers: sdiff -l file1.txt file2.txt | cat -n

>> PYTHON: Unit testing / changing log level in unit testing / pytest change logging level / logger level:
   def test_thing(caplog):   # caplog is a special term
       caplog.set_level(logging.ERROR, "your_logger_name_as_string")  # make it less verbose (errors only)

>> PYTHON: import a directory with code in it so you can run some debugging code as if you're in the full codebase: Top of the file:    import sys   and   sys.path.insert(1, "/Users/YOURNAME/YOUR_PATH_HERE")

>> PYTHON: get dictionary with SOME keys remove / remove keys from dictionary: import toolz --> toolz.dicttoolz.dissoc(the_dict, the_keys_to_remove)
>> PYTHON: Get today's date / time in Pacific Time: from datetime import datetime // import pytz // dateime.datetime.now(tz=pytz.utc).astimezone(pytz.timezone('US/Pacific'))  (plus .date().isoformat() if you want a string)   
>> PYTHON: Jupyter auto-reload a module before each run: %load_ext autoreload (newline)  %autoreload 2  from foo import some_function (<-- the import is only needed ONE time now!)  
>> PYTHON logging: RED error messages, but only if we are in the 'DEBUG' (detailed output) mode:  logging.error('My message')      and then:                       if logging.getLogger().isEnabledFor(logging.DEBUG):   logging.error(f'MORE {e}')  
>> PYTHON debug a crashing program (worse):  python -m pdb YOURPROG -some -args -here . 'c' continues execution, 's' steps.  
>> PYTHON debug a crashing program (better): jupyter --pdb YOURPROG --  -more -args -here  'c' continues execution, 's' steps.  
>> PYTHON debug a crashing program by enterint the debugger at a specific point:  import pdb; pdb.set_trace()  
>> PYTHON / PIP install locally to home directory -> .local: pip2.7 install --user PACKAGENAME  
>> PYTHON / PIP install locally to home directory -> .local: pip2.7 install --ignore-installed --install-option="--prefix=" PACKAGENAME  
>> PYTHON debug a crashing program: Put this in the code to enter the python debugger on a line:  import pdb; pdb.set_trace()  
  
>> BIOM format: convert FROM BIOM   : biom convert -i INPUT.biom -o TABDELIM.txt  --to-tsv  --header-key=taxonomy  
>> BIOM format: convert BACK TO BIOM: biom convert -i TABIN.txt  -o OTUTABLE.biom --to-hdf5 --table-type="OTU table" --process-obs-metadata taxonomy  
>> R: Unload all packages: lapply(paste('package:',names(sessionInfo()),sep=),detach,character.only=T,unload=T)  
>> R: set vector or list names by VARIABLE instead of plain text: Since you cannot do x='first'; y='second'; c(x='a', y='b'), you must instead do this: setNames(c('a','b'), c(x, y))  (which will set the names accordingly, in a single command. 
>> R: RMD/KNITR render RMD headless: script -e 'rmarkdown::render("/path/to/the.Rmd", "html_document")'  (or you can specify output_file='path_to_output_file.html')    
>> R: RMD/KNITR: write markdown VERBATIM in R ```{r results='asis'} (newline) cat('\n\n## HERE is a programmatically-defined SECTION in Rmd\n')   
>> R: A 'stop' and 'stopifnot' that actually work in RStudio:  stop_infloop   <- function(msg) { stop(msg); while(TRUE) {} }           and         waitifnot <- function(cond) { if (!cond) { stop_infloop(paste(deparse(substitute(cond)), ' is not TRUE.')) } }     
>> R: List (same # elements per item) --> Data frame (list to data.frame / list to table): do.call(rbind.data.frame, YOUR_LIST_HERE)    
>> R: Save copy of figure as pdf: dev.copy2pdf()  
>> R: Convert columns of a dataframe to factors (from char): fact.df   <- char.df; fact.df[] <- lapply(char.df[], as.factor)  
>> R: Prevent blank page in PDF: pdf(..., onefile=FALSE)  
>> R: Aggregate rows / average rows with same name (mean): mAGG <- t(sapply(by(MAT,rownames(MAT),colMeans),identity))  
>> R: Read bzip/gz file:  x <- read.table(gzfile("name.bz2") (gzfile supposedly handles .txt/.xz/.bz2/.gz)  
>> R: Read bzip/gz file line by line:  conn=gzcon(file('in.txt.gz','r')); while(TRUE) { lines22.vec=readLines(conn,n=22); if (0==length(lines22.vec)){break; } } close(conn);  
>> R: Read from stdin:    x <- read.table(pipe("cat something name.bz2 | bunzip2")  
>> R: traceback() - to properly debug. Also: options(error=recover())  
>> R: t.test failing? Use tryCatch: tryCatch(something, error=function() { return('NA'); });  
>> R: Need to un-list things? Use unlist!  
>> R: PLOT: Perpendicular axis labels: par(las=2) (1 = horiz, 3 = vertical)  
>> R: PLOT: draw beyond the axes / off / outside the graph region: par(xpd=T/F/NA)  
>> R: PLOT: square plot: par(pty='s')  
>> R: PLOT: heatmap colors: color_function = grDevices::colorRampPalette(c('cyan', 'blue', 'black', 'red', 'yellow')); color_function(n=10)  
>> R: PLOT: heatmap colors: COOL: black->blue->cyan->yellow:  grDevices::colorRampPalette(c('black', 'blue', '#0099FF', 'yellow'))(n=10)                        <-- note the 10 here  
>> R: PLOT: heatmap colors: COOL, longer ramp:                grDevices::colorRampPalette(c('black', '#000066', 'blue', '#0099FF', '#BBCC00', 'yellow'))(n=10)  <-- note the 10 here  
>> R: PLOT: heatmap colors: WARM: black->red->orange->yellow: grDevices::colorRampPalette(c('black', 'red', 'yellow'))(n=10)                                    <-- note the 10 here  
>> R: PLOT: pheatmap symmetrical around zero: library(pheatmap); symcol = colorRampPalette(c('orange','black','cyan'))(n=5); bounds = 3; pheatmap(DATA, color=symcol, breaks=seq(from=-bounds, to=bounds, length.out=length(symcol)+1), border=NA, cluster_rows=T, cluster_cols=T, fontsize_row=5);    
>> R: PLOT / PHEATMAP: nice heatmap part 1: library(pheatmap); pheatmap(DATA,  color=grDevices::colorRampPalette(c('black', 'red', 'yellow'))(n=10),   border=NA,  cluster_rows=T,  cluster_cols=F,  main='Heatmap',  fontsize_col=9,  fontsize_row=9,  display_numbers=T,  fontsize_number=4);    
>> R: PLOT / PHEATMAP: nice heatmap part 2: library(pheatmap); library(viridis); pheatmap(DATA, color=viridis::magma(20), border=NA, cluster_rows=T, cluster_cols=F, main='Heatmap', fontsize_col=9, fontsize_row=9, display_numbers=F, fontsize_number=4);    
>> R: PLOT: pheatmap clustering PART of the heatmap only:   cluster_and_return_ids = function(dd, distmethod='euclidean', linkage='complete') {    if (nrow(dd) == 1) { return(rownames(dd)) } else if (is.null(dd) || nrow(dd) == 0) { return(c()) }  dists=dist(dd, method=distmethod);  clusts = hclust(dists, method=linkage);  return(clusts[clusts]) };  lab.1 = cluster_and_return_ids(l2n.mat[probe.annot.df == 'TYPE1', , drop=F]);    lab.2 = rownames(l2n.mat)[probe.annot.df != 'TYPE1'];   reordering_labels.vec = c(lab.1, lab.2);   l2n.mat = l2n.mat[reordering_labels.vec, ]             
>> R: PLOT: pheatmap with ROW labels (the trick is that the *rownames* must match, which is not true for column annotation!): library(pheatmap); m=matrix(rnorm(100),ncol=5); rownames(m) <- paste('R',1:nrow(m)); rannot=data.frame('RANNOT1'=c(rep('A',10), rep('B',7), rep('C',3)), row.names=rownames(m)); pheatmap(m, annotation_row=rannot, annotation_colors=list('RANNOT1'=c('A'='red','B'='blue','C'='green'))); NOTE: pheatmap annotation frequently does NOT like NAs or non-character columns, and will crash with baffling messages. Try this to character-ize your row annotation: Example:  annot.sanitized[ , ] = lapply(rannot[, ], as.character); annot.sanitized[is.na(annot.sanitized)] = 'NONE' # text, not an NA value  
  
>> R: GGPLOT2: side-by-side barplot without removing 'zero-item' categories (i.e. a blank spot for a missing category): ggplot(DATA, aes(x=CATEGORY, fill=US_STATE)) + geom_bar(position=position_dodge(preserve="single")) +  
>> R: GGPLOT2: geom_boxplot: ggplot(dat, aes(x=gene_name, y=EXPRESSION)) + geom_boxplot(aes(fill=repClass), outlier.alpha=0.5, outlier.size=0.5) +  
>> R: GGPLOT2: add line of best fit:   geom_smooth(method='lm', col='red') + ... (by default, uses the same x and y as in the ggplot setup)   
>> R: GGPLOT2: omit the legend for only CERTAIN parameters / aesthetics:   + guides(color=FALSE) or + guides(fill=FALSE)  . Useful if there are just way too many categories. 
>> R: GGPLOT2: custom FILL colors (or outline color with just 'scale_color_manual'): scale_fill_manual(values=c('#999999', '#E69F00', '#56B4E9'))  
>> R: GGPLOT2: rotate axis labels (x-axis perpendicular): YOURGGPLOT + theme(axis.text.x = element_text(angle=90, vjust=1, size=12, hjust=1))   
>> R: GGPLOT2: multi plots (facets): YOURPLOT + facet_grid(. ~ DRUG_TREATMENT, scales='free_x', space='free_x') <-- replace the '.' for a 2-way facet. The format is: x ~ y, with a '.' for an unused facet dimension.  
>> R: GGPLOT2/DPLYR: pairwise correlations of everything BY GROUP in a melted matrix:  melted.tib %>% group_by(SCHOOL_TYPE, US_STATE) %>% do(data.frame(Cor=t(cor(.[['TEST_SCORE']], .[['STUDENT_AGE']] , method='spearman' ) )))     
>> R: correlation plot (does not work with ggsave, though, you have to use pdf() or png()): pairwise correlations of a matrix, visualized: library('corrplot'); cors = cor(data.df, method='spearman'); corrplot(cors, method='square', order='hclust', hclust.method='complete', addgrid.col='#00000000', rect.lwd=1, tl.cex=0.5, tl.col=c('red','blue'), tl.srt=90);     
>> R: GGPLOT2/DPLYR/RESHAPE: melt an input matrix with MULTIPLE ANNOTATION COLMNS (not just one!): your_tibble %>% reshape2::melt(id.vars=c('GeneName','ProteinType','CodeClass','OtherID'), variable='SAMPLE_COLUMN', value.name='EXPRESSION_NEW_OUTPUT_COLUMN')  
>> R: GGPLOT2: melted sample data: m1=matrix(runif(80),nrow=10, dimnames=list(paste('Gene',1:10), LETTERS[1:8])); library(dplyr); library(reshape2); melted = reshape2::melt(m1, value.name='EXPRRESSION') %>% `colnames<-`(c('GENE','SAMP','EXPR'))  
>> R: DPLYR: how to CAST (dcast) a melted dataset back into a matrix-like structure: x.melt = YOUR_LONG_DATA_STRUCTURE; reshape2::dcast(x.melt, US_STATE ~ YEAR, value.var='annual_income'))  
>> R: GGPLOT2: equivalent of par(pty='s') (square plots with X / Y axes the same size): YOURPLOT + coord_fixed()  
>> R: GGPLOT2: scatterplot: ggplot(melted.df, aes(x=CATEGORY)) + geom_bar(aes(weight=VALUE, fill=CATEGORY))  
>> R: GGPLOT2: barplot basic example: ggplot(melted.df, aes(x=CATEGORY)) + geom_bar(aes(weight=VALUE, fill=CATEGORY))  
>> R: GGPLOT2: barplot reordered by GROUP2, but still plotted by individual 'SOMETHING1': ggplot(melt.df, aes(x=reorder(SOMETHING1, as.integer(as.factor(GROUP2))))) + geom_bar(aes(weight=VALUE, fill=GROUP2))  
>> R: GGPLOT2: barplot reordered AND 'faceted' by another group: ggplot(melt.df, aes(x=reorder(MAINCATEGORY, 0.5*OTHERGROUP+0.1*THING) )) + geom_bar(aes(weight=VALUE, fill=OTHERGROUP)) + theme_minimal() + theme(axis.text.x = element_text(angle=90, vjust=1, size=12, hjust=1)) + facet_grid(. ~ OTHERGROUP, scales='free_x', space='free_x')  
>> R: GGPLOT2: barplot reordering example: reorder_by_size <- function(x) { factor(x, levels=names(sort(table(x)))) }; ggplot(mpg, aes(reorder_by_size(class))) + geom_bar()  
>> R: GGPLOT2: heatmap-ish (not as good as pheatmap, and does NOT cluster): ggplot(data %>% mutate(VALUECOL=log2(VALUECOL)), aes(x=SAMPLENAME, y=GENENAME)) +  geom_tile(aes(fill=VALUECOL)) +  scale_fill_gradientn(colors=c('red','white','blue'), name='Description goes here') +  theme_minimal() +   theme(axis.text.x = element_text(angle=90, vjust=1, size=12, hjust=1))  
>> R: GGPLOT2: histogram: ggplot(data %>% mutate(VALLOG=log2(VALLINEAR)), aes(VALLOG)) + geom_histogram(bins=30, col='#0000FF33', fill='#FF000088') + labs(title='Hist', x='Val', y='Count') + theme_minimal()  # or breaks=seq(10,20,by=5) if you want to manually specify the bin breaks  
>> R: GGPLOT2: histogram ALTERNATE method of log scaling: ggplot(data %>% mutate(VALLOG=log2(VALLINEAR), aes(VALLOG)) + geom_histogram(bins=30, col='#0000FF33', fill='#FF000088') + scale_x_log10() + scale_y_log10() + labs(title='Hist', x='Val', y='Count') + theme_minimal()  
  
>> R: PLOT: totally blank plot: plot(c(0,1),c(0,1),ann=F,bty='n',type='n',xaxt='n',yaxt='n')  
>> R: multiply/divide/'sweep' a matrix by a vector, line by line / row by row: sweep(yourmatrix, 2, yourvec, '/'); stopifnot(ncol(yourmatrix)==length(yourvec)); <-- replace '/' with '*' to multiply  
>> R: find attributes of a variable: attr(x, 'theAttribute'); Works when names()/attributes() fails!  
>> R: inspect object: str(...)  
>> R: cut a data structure (e.g. a vector) into bins, like a histogram: cut(...)  
>> R: factor -> Integers: unclass(...)  
>> R: Collapse a list down to a basic vector: unlist(...)  
>> R: Remove names from a structure: unname(...)  
>> R: Resize terminal width / columns: options(width=Sys.getenv("COLUMNS"))  
>> R: write.table(DATA, file='x.txt', row.names=T, col.names=NA, sep='\t', quote=F)   
>> R: make libraries usable by non-root users: sudo chmod -R a+r /usr/local/lib/R ; sudo find /usr/local/lib/R -type d | sudo xargs chmod a+x  
>> R: run in the queue: qplz "R CMD BATCH script.R" or RScript, but be sure to 'library(methods)' if you use horrendous Rscript  
>> R: Mac: read from clipboard: y=data.matrix(read.delim(pipe("pbpaste")));  
>> R: PACKAGES: Pick CRAN mirror, no GUI popup (install.packages): chooseCRANmirror(graphics=F)  
>> R: PACKAGES: Install package from source:  R CMD INSTALL packagename.tar.gz  (or use biocLite(...) or install.packages(...))  
>> R: PACKAGES: sessionInfo(): show packages and versions of installed packages. ls('package:something') shows a summary of package contents, and help(library='packagename') will show package contents in more detail. '.libPaths()' shows lib dirs.  
>> R: PACKAGES: update all installed packages: update.packages(ask=FALSE, checkBuilt=TRUE)  
>> R: PACKAGES: install package from BIOCONDUCTOR without root: source('http://www.bioconductor.org/biocLite.R'); biocLite(lib=head(unlist(strsplit(Sys.getenv('R_LIBS'),':')),1), lib.loc=head(unlist(strsplit(Sys.getenv('R_LIBS'),':')),1), pkgs=c('PACKAGENAMEHERE'))  
>> R: PACKAGES: install package from CRAN without root: install.packages(repos='http://cran.cnr.berkeley.edu/', dependencies=TRUE, lib=head(unlist(strsplit(Sys.getenv('R_LIBS'),':')),1), pkgs=c('PACKAGENAMEHERE'))  
>> R: BED->GRanges: BED file to GeneRanges / GenomicRanges / GRanges object: use 'rtracklayer': library(rtracklayer); gr = rtracklayer::import(rmsk_bed, format='bed'); stopifnot('GRanges' %in% (class(gr))[1]); stopifnot(all(start(gr)<=end(gr)))  
  
>> R/DPLYR/TIBBLE/DATAFRAME: replace NA with a value:   STRUCTURE %>% base::replace(., is.na(.), 0.00)  
>> R/DPLYR/TIBBLE: round a column:   TIB %>% mutate(NEWCOL=signif(OLDCOL,2)  
>> R/DPLYR/TIBBLE: get the top element per category. Here, top income per US county on a per-state basis:   TIB %>% group_by(US_STATE) %>% arrange(desc(INCOME_PER_COUNTY)) %>% filter(row_number()==1)  . For top and bottom, you could say filter(row_number()==1 | row_number()==n())  . However, note that that will always return only ONE row, whereas we might want two in the case where a state had only one county, in which case we would want: ... %>% dplyr::slice(c(1,n()))  <-- this will double-return the same row for both 'top' and 'bottom' if there is only one row. Do not omit the 'dplyr::' or else you will get the wrong SLICE function!  
>> R/DPLYR/TIBBLE: RENAME a column to a variable: tib %>% dplyr::rename(!!variable_goes_here := 'Literal_Column_Name_To_Replace'); <-- note that 'rename' and 'select' both can be overridden by other packages, and also have a dplyr variant with TOTALLY DIFFERENT BEHAVIOR      
>> R/DPLYR/TIBBLE: READR read QUIETLY / not verbose: readr::read_tsv(..., progress=F, col_types=cols()); <-- super non-intuitive way to suppress printing      
>> R/DPLYR/TIBBLE: split tab-delim fields into a TABLE / matrix: tempsplits = sapply(YOUR_CHAR_DATA_OF_INTEREST, strsplit, '\t'); final.tib = as_tibble(do.call(rbind, lapply(tempsplits, rbind)));   OR even better, you can use 'stringr' and: as.data.frame(t(do.call(cbind, stringr::str_split(YOUR_CHAR_VECTOR, pattern='\t'))));  
>> R/DPLYR/TIBBLE: Mutate only CERTAIN columns: my.tib %>% mutate_at(vars(matches('(snakes|cakes)')), funs(log2(1 + .) ))  
>> R/DPLYR/TIBBLE: Aggregate a small number of rows in a tibble: mytib %>% group_by(MYGRPCOL) %>% summarize(x=mean(OTHERVALUE), n=n()) %>% ungroup()  
>> R/DPLYR/TIBBLE: Aggregate multiple rows BY A PASSED-IN-VARIABLE in a tibble (way 1 of 3): mytib %>% group_by(MYGRPCOL) %>% summarize_at(.vars=c('colA','colB','colC'), .funs=c('NEWSUM'='sum', 'NEW_MEDIAN'='median')) %>% ungroup()  
>> R/DPLYR/TIBBLE: Aggregate multiple rows BY A PASSED-IN-VARIABLE in a tibble (way 2 of 3): mytib %>% group_by(MYGRPCOL) %>% summarize_at(vars(matches('^COUNT_')), funs('FINALSUM'=sum, 'FINALMEAN'=mean)) %>% ungroup()  
>> R/DPLYR/TIBBLE: Aggregate multiple rows BY A PASSED-IN-VARIABLE in a tibble (way 3 of 3, probably the best) if you also want the N elements in each group (n()): mytib %>% group_by(MYGRPCOL) %>% mutate(NCOUNT=n()) %>% group_by(MYGRPCOL, NCOUNT) %>% summarize_at(vars(matches('^COUNT_')), funs('FINALSUM'=sum, 'FINALMEAN'=mean)) %>% ungroup()  
>> R/DPLYR/TIBBLE: tibble to data frame with rownames:    df  = tibble::column_to_rownames(as.data.frame(tib), var='COLUMN_NAME_THAT_BECOMES_ROWNAME')   
>> R/DPLYR/TIBBLE: data frame (with rownames) to tibble:  tib = tibble::rownames_to_column(df, 'New_Column_Name_From_Rowname')  
>> R/DPLYR/TIBBLE: set rownames in dplyr in a data frame:  tib %>% `rownames<-`( YOUR_ROWNAME_VEC )  <-- note the parenthese and backticks!  
  
>> GZIP: verify files: for f in **/*.gz; do gunzip --test -v $f ; done;  
>> LINE WRAP: wrap lines at 80 chars, preserving/ignoring word boundaries: fold -w 80 -s text.txt (omit -s to cut words)  
>> SHELL/BASH/ECHO: echo to STDERR instead of STDOUT: echoerr() { echo "$@" 1>&2; }  usage: echoerr 'mystring'  
>> SHELL/BASH: Redirect stdout/stderr separately: CMD > out.txt 2> err.txt  
>> SHELL/BASH: Pipe both STDERR and STDOUT: CMD |& less  or the old way --> CMD 2>&1 (for bash < 4) 
>> SHELL/BASH: Modify variable: newV=$(sed -e's/a/1/; s/b/2/; s/c/3/' <<< $oldV)  
>> SHELL/BASH: Script run with ionice:  ionice -c3 -p$$ (top line of a /bin/sh script)  
>> SHELL/BASH: Rename files with parent directory in filename for f in DIRS*/file.whatever; do echo $f $(dirname $f)/$(dirname $f).$(basename $f).whatever ; done  
>> SHELL/BASH: Rename files to NOT have the path in them at all: for fff in **/*.html ; do ggg=; mv  ; done  
>> SHELL/BASH: Rename files to have unique names: for f in **/*fastqc.html; do /bin/cp $f DESTINATION_DIR/$(md5sum $f | cut -d ' ' -f 1).$(basename $f); done  
>> SHELL/BASH: Rename files to a LIST of new names in a file: while read -u 9 SRC; do read -u 8 DEST; echo mv "$SRC" "$DEST"; done 9<<<"$(ls -1 * | grep -v NEW_FILENAME_LIST.TXT)" 8< NEW_FILENAME_LIST.TXT  <-- note that NEW_FILENAME_LIST.TXT must be a SINGLE COLUMN of text in the same order as the files to rename. And the 'grep -v' part there is so that the filename list itself does not mess up the renaming.  
>> SHELL/BASH: Check to see if dir is empty if [[ "$(ls -A $OUTDIR)" ]]; then echo "ERR: Output dir not empty!"; exit 2; fi  
>> MD5 on EACH LINE OF A FILE (Perl) (note: WITHOUT the newline): cat YOURFILE | perl -e 'use Digest::MD5 qw(md5_hex); while(my $line=<>){ chomp($line); print(md5_hex($line).qq{\n}); }'     
>> MD5 verify: md5sum -c md5*.txt. Checksum must be 1st col, then 2 spaces, then filename. If checksum is 2nd: cat md5*.txt | awk '{print $2"  "$1}' | md5sum -c  
>> MD5 on multiple files: find . -type f -exec md5 {} \; > list.txt     
>> HEX HEXDUMP: View bytes in hex form / hex editor: hexdump -v -C FILENAME (the '-v' prevents 'squeezing' of contiguous NULL bytes)  (or use colorized command-line viewer 'hexyl -v' or 'hex on the Mac (brew install hexyl))  
>> UNIX get Pacific time:    TZ=America/Los_Angeles date       (date might normally give you UTC, or whatever the system is set to)  
>> UNIX detect null bytes at the start of a file: for x in *; do echo -e -n "$x\t" ; od -v $x | head -n 1 ; done | less -S  (you can grep for '000000 000000 (etc)' in the output   
>> AWK: sum numbers in column 1 of a file: cat FILE | awk '{ sum+=$1} END {print sum}' 
>> AWK: Get only FIRST line with a unique occurrence in column 1. Does not require sorting: cat FILE | awk -F'\t' '!_[$1]++' (assumes separator is tab)  
>> AWK FILTER based on numeric value (like select.pl): filter based on a number: cat test | awk '{ if ($1 >= 900 && $1 > 0) print; }'  

>> GIT: show all commits / historical commits to master (or any other branch) for all time: git log --first-parent master

>> GIT: pick only VERY SPECIFIC line changes from another branch (even more granular than cherry-pick / cherry pick):
         git cherry-pick --no-commit [SOURCE_COMMIT_HASH_HERE]
         git reset   # un-stage changes from that commit
         git add -p  # interactively select the changes that you want
         git commit  # finish up

>> GIT interactively merge / cherry-pick 'git add -p' from ANOTHER BRANCH: first switch to your pristine branch (git checkout master -> git checkout -b mybranch), then use cherry-pick -n:  git cherry-pick --no-commit OTHER_COOL_BRANCH; then you need to git reset each file, and finally git add -p .  to INTERACTIVELY 'git add -p' from another branch


>> GIT: move branch / git rename branch: git branch -m OLD NEW . Or if you want to rename the CURRENT branch, just git branch -m NEWNAME
>> GIT: undo commit / undo last commit / rollback last commit roll back: git reset --soft HEAD~1
>> GIT: verbose debugging:  GIT_SSH_COMMAND='ssh -vvv' git fetch   <-- or whatever command you want    
>> GIT: change your git commit messages, say, 5 commits ago: git rebase -i HEAD~5 and then change the 'command' on the left to REWORD. DO NOT EDIT DIRECTY IN THE EIDTOR UNTIL AFTER YOU SAVED AND EXITED  <-- 5 can be whatever  
>> GIT: checkout file from other branch AND OVERWRITE LOCAL COPY:  echo 'overwriting local copy ' && git checkout OTHERBRANCH -- FILENAME  
>> GIT: change the 'upstream' of your branch to a REMOTE upstream: git branch YOURLOCALBRANCH --set-upstream-to origin/SOME_OTHER_BRANCH   <-- the 'origin' part seems necessary to make the upstream the remote  
>> GIT: do not download huge LFS files when git lfs is enabled: GIT_LFS_SKIP_SMUDGE=1 git clone YOUR_REPO  
>> GIT: revise a commit that is ALREADY a pull request:  once you have your changes committed, send it upstream with --FORCE, with: git push -f origin HEAD:refs/heads/whatever_your_pull_request_branch_was   
>> GIT: only get the last 1 revision (faster): git clone --depth=1 https://your-site-here/something.git  
>> GIT: discard/revert ALL local changes TO ALL FILES: DANGER >> git reset --hard origin/master << DANGER (this also works for multi-path names, like --hard origin/coworker/beta_branch/coolfeature123 (note that ORIGIN is still at the beginning there!)  
>> GIT: push local branch to remote branch with DIFFERENT NAME: to set up TRACKING only (upstream parent): git push --set-upstream origin LOCALNAME:REMOTENAME   Or to push it:   git push -f origin my_local_branch:different_named_remote_branch (or if you want, just say push -f origin HEAD:other_remote_branch if it's your current HEAD)  
>> GIT: discard/revert local changes to JUST ONE FILE: git checkout -- FILENAME_HERE  
>> GIT: fix an HTTP repo to SSH: git remote set-url origin git@github.com:RepoNameHere/ProjName.git  
>> GIT: see old version of file (example: ~4 = 4 revisons ago): git show HEAD~4:./some_file.txt  
>> GIT: delete a REMOTE branch: git push origin --delete BRANCHNAME (not delete, surprisingly)  
>> GIT: list files in a branch: git ls-tree -r -l BRANCHNAME  
>> GIT: diff commits / commits in one branch but not another:   git cherry -v YES_IN_THIS_BRANCH   BUT_NOT_THIS_BRANCH   | grep "^\+"    (order matters)
>> GIT: rebase with MY files, overwriting the merge conflicts: git checkout --ours /path/to/my/conflicting/dir/or/file/  
>> GIT: what changed between branches?: git diff --name-only BRANCH1 BRANCH2  (do not confuse this with --summary, which ONLY shows added/deleted files)  
>> GIT: what changed in ONE SPECIFIC FILE between branches?: git diff master -- fileX.py    coolbranch(optional)  ./fileX.py  
>> GIT: solve 'your branch is ahead of master: 1. See what is different: git diff master origin/master     If you seriously didn't make any changes that you want to keep in your local copy, then obliterate your local changes with: git reset --hard origin/master  
>> GIT: GUI and 'when was a branch made from master': gitk --all --select-commit=$(git merge-base YOURBRANCH master)   
>> GIT: when branch was last used (MINIMAL): for BRANCH in $(git branch -r | grep -v 'HEAD'); do echo -e $(git show --format='%ci %cr' $BRANCH | head -n 1) \\t$BRANCH; done | sort -r  
>> GIT: when branch was last used AND last commit (fancy): git for-each-ref --sort=committerdate refs/heads/ --format='%(HEAD) %(color:yellow)%(refname:short)%(color:reset) - %(color:red)%(objectname:short)%(color:reset) - %(contents:subject) - %(authorname) (%(color:green)%(committerdate:relative)%(color:reset))'  
>> GIT check a specific file and its history:  git log --follow -p FILENAME 
>> GIT merge but keep LOCAL changes and not the remote ones: git pull PARENT_BRANCHNAME -Xours  or  git merge PARENT_BRANCHNAME --strategy-option ours  
>> GIT interactively merge ONE FILE ONLY: git checkout --patch origin/master PATH/TO/YOUR/FILE.TXT   
>> GIT interactively edit/squash/combine/rebase a bunch of commits / edit commit messages: git rebase -i HEAD~[N] <-- where N is the number of commits you're interested in... you can put any number here, like 10 or whatever. [s]quash is how you combine commits, by the way.  
>> GIT grep for a particular piece of code / line of source code that changed in a file:  git grep 'ESRD' $(git rev-list --all -- PATH/TO/FILE.py ) -- PATH/TO/FILE.py
>> GITHUB solve 'Your push would publish a private email address': git config --global user.email 'YOURGITHUB_USERNAME_GOES_HERE@users.noreply.github.com'; git commit --amend --reset-author; git push ; (and make sure you don't have a .git/config that overrides this setting) 
>> GITHUB: to solve 'Please make sure you have the correct access rights', usually you can fix this on the Mac with:  ssh-agent -s &&  ssh-add -K && ssh-add  . Not sure if all of those are necessary. This problem seems to occur after system updates.  
>> GIT / GITLAB: change author of a commit: git commit --amend --author="YOURNAME_HERE <YOUREMAIL@EXAMPLE.COM>" --no-edit
>> TERM: Get terminal width/height: tput cols  or  tput lines  
>> BACKUPS: Check for changed files of a certain name: grep -E '>f.' /it/backup/log/*.log | grep YOURNAME  
>> UNIX/FILE: Add filename to top of (copy of) files. for f in FILES*; do echo $f | cat -  > NEW_$f ; done     ...or if you will later paste/join those: for f in SPLIT*.diff; do echo $f | cat - $f | table-no-ragged.py - > NEW_$f; done  
>> UNIX: SYMLINKS: convert RELATIVE symlinks to ABSOLUTE PATHS: for fff in *; do FULL=$(readlink -f $fff); ln -sf $FULL $fff ; done  
>> UNIX: SYMLINKS: 1) double-dereference symlinks, showing their final targets: find -L . -type f -exec readlink -f {} \;  
>> UNIX: SYMLINKS: 2) double-dereference symlinks, showing their final targets: for f in * ; do echo -e $f"\t"$(readlink -f $f); done    
>> UNIX: SYMLINKS: Just delete ALL symlinks: find DIRNAME/ -type l -delete  
>> UNIX: SYMLINKS: print orphaned/broken symlinks: find . -type l -exec sh -c "file -b {} | grep -q ^broken" \; -print  
>> UNIX: SYMLINKS: set up symlinks from TimeForScience: ln -sfn ~/TimeForScience/Config/Alex_Williams/.??* ~/  
>> UNIX: SYMLINKS: executables: for f in $(find PATH -maxdepth 1 -perm -111 -type f); do ln -s $f ./; done  
>> UNIX: total file size for files with a certain extension (Here, jpg): find . -type f -iname '*.jpg' -print0 | du -ch --files0-from=-  
>> UNIX: list files w/modified time MORE THAN one week ago: find . -mtime +7  
>> UNIX: list files w/modified time LESS THAN one week ago: find . -mtime -7  
>> UNIX: list files w/modified time two days ago: find . -mtime 2  
>> UNIX: list files w/modified time between 6 and 9 minutes ago: find . -mmin +5 -mmin -10  
>> UNIX: list ONLY dot files, no .. or up a directory: ls -l .??*  
>> UNIX: upgrade: sudo aptitude update && sudo aptitude safe-upgrade <-- more assertive than apt-get upgrade. 
>> UNIX: IO nice (low-priority): ionice -c2 -n7 -pPROCID <-- -n7: lowest non-idle priority. PROCID is the PID. 
>> UNIX: IO nice (lowest-priority): ionice -c3 -pPROCID  <-- -c3: IDLE priority; doesn't slow anyone else down. 
>> UNIX: IO nice for a user $THEUSER: SSS=$(ps -fu $THEUSER | perl -pe 's/[ ]+/\t/g' | cut -f 2 | tail -n +2); for pid in $SSS ; do echo 'PID' $pid; sudo ionice -c3 -p $pid ; sudo renice +10 -p $pid; done 
>> UNIX: Find zombie processes: ps aux | awk '{ print $8 " " $2 }' | grep -w Z  
>> UNIX: Check Linux version: cat /etc/*-release  
>> UNIX: Set system time on Ubuntu: sudo ntpdate-debian  
>> UNIX: Get system DATE only in ISO format on the Mac or UNIX: date "+%Y-%m-%d"  
>> RSYNC to amazon instance with an ssh key: rsync -avz -e "ssh -i ~/pems/your_pem.pem" LOCALFILE ec2-user@REMOTEINSTANCE.COM:  <-- DO NOT FORGET THE ':'!!!  
>> RSYNC transfer files: rsync --dry-run -R -havz --progress --stats --bwlimit=999999 LOCALFILES USER@REMOTE.COM:/path/to/   
>> RSYNC local Mac preserve all permissions and copy Mac extended attributes EXACTLY with DELETING: sudo rsync --DANGER__delete_DANGER -a -H -A -X -v /SOURCE_DIR /Path/To/Your/Destination/DIR <-- double check the '/' parts  
>> UNIX: Password FTP command line Xfer: wget -r ftp://USERNAME:password@ftp.some.site.com/Somefiles  
>> UNIX/WGET/TRICKLE: Rate limit any command to 500kbps (-d = down, -u = up): sudo trickle -d 500 -u 500 ncftp 'your_ftp_site_here'  
>> UNIX: See why Ubuntu wants to restart:  cat /var/run/reboot-required.pkgs  
>> UNIX/TOP/USAGE/HTOP: CPU usage by user: top -b -n 1 -u $USER | awk 'NR>7 { sum += $9; } END { print sum; }'  
>> UNIX SCREEN: did your arrow keys stop working in 'less' within 'screen'? Detach the screen (modifier+d), then run 'reset' on the command line in the outer shell.  
>> UNIX SCREEN: full screen a tiny window: modifier + F (capital F) for 'fill'.  
>> UNIX GREP: filter files based on presence of text: mkdir YES NO; for f in *.txt; do if [[ "$(grep 'YOUR_STRING_HERE' $f | wc -l)" -ne "0" ]]; then mv $f yes/; else mv $f no/; fi ; done  
>> UNIX GREP: find text in a file VERBATIM, like join: grep -w -f KEYFILE.txt search_in_file.txt  (remove the '-w' to allow PARTIAL matches as well. You can use '-E' to allow regular expressions in the file too!) 
>> UNIX GREP Count occurrences of a match per line: cat FILE | grep -o -n SEARCHTERM | cut -d : -f 1 | uniq -c  
>> BSUB change queue that a job is in: bswitch LONGQUEUE <YOUR_JOB_ID_HERE>  
>> QSUB/QUEUE/PBS submit job: echo 'echo "hello I am a test job"' | qsub -q Bio -W group_list=YOURQUEUE ; sleep 10; cat STDIN*;  
>> QSUB/QUEUE/PBS: Delete all your jobs one by one: for j in $(qstat -a | grep 'YOURNAME' | cut -d '.' -f 1); do echo "qdel $j..."; qdel $j ; done  
>> QSUB/QUEUE/PBS: restart torque: sudo service pbs_sched stop ; sudo service pbs_mom stop ; sudo service pbs_server stop ; sudo /bin/rm /var/lock/subsys/pbs_server ; sudo service pbs_sched start ; sudo service pbs_mom start ; sudo service pbs_server start ; qstat  
>> QSUB/QUEUE/PBS: info on the queue: qstat -Qf or qstat -B more details  
>> QSUB/QUEUE/PBS: qrun MULTIPLE jobs in a range: for f in $(seq 119905 119919); do sudo $(which qrun) $f; done  
>> QSUB/QUEUE/PBS: kill all YOUR jobs: qselect -u $USER | xargs qdel ;  
>> QSUB/QUEUE/PBS: see available nodes / available queue resources:  pbsnodes -a   (gives a summary)   or   pbsnodes -a -v (per 'socket')  or  qstat -Qf (per-queue amounts  
>> UNIX/DPKG/APT-GET: dpkg/apt-get woes? Try manually editing /var/lib/dpkg/info/YOURPACKAGE . Be careful!  
>> UNIX/DPKG/APT-GET: Check the version of an apt-get installed package:   dpkg -s <packagename>  OR   dpkg -l | grep -i <search_string>  
>> UNIX/YUM/RPM install: (this NEVER works, but): rpm -Uvh your.rpm . Does NOT handle dependencies. 
>> YUM list installed packages: yum list installed "*perl*"  <-- stars are important!     
>> UNIX/APTITUTDE: sudo aptitude update && sudo aptitude safe-upgrade <-- more assertive than apt-get upgrade. 
>> UNIX/APT-GET: upgrade only EXISTING packages matching wildcard pattern: apt-get install --only-upgrade 'r-cran*'  
>> UNIX/APT-GET: Fix 'NO_PUBKEY *SOMEKEY*' in APT: gpg --keyserver subkeys.pgp.net --recv *SOMEKEY* ; gpg --export --armor *SOMEKEY* | sudo apt-key add -  
>> UNIX SSH/SHELL/LOGIN to a 'fresh' shell without loading bashrc or bash_profile: ssh -t user@server bash --norc --noprofile  
>> UNIX SSH Passphraseless Option 1 of 2: Client: ssh-keygen -t rsa ; Append client ~/.ssh/id_rsa.pub   to server ~/.ssh/authorized_keys  
>> UNIX SSH Passphraseless Option 2 of 2: Client: On your CLIENT, you can copy your existing SSH profile with just: ssh-copy-id your_username@server_goes_here  
  
>> ZIP: Zip a folder: zip -r ARCHIVENAME FOLDER  
  
>> GNU MAKE with a bash loop (5 lines, look for text 'MAKELOOP'):  
>> MAKELOOP1: looped: INFILE.txt                    
>> MAKELOOP2:         while read -r X; do     \    
>> MAKELOOP3:                 echo $$X; \    
>> MAKELOOP4:         done < INFILE.txt;      \    
>> MAKELOOP5:         true                          
>> MAKEFILE: set variables in rule:   $(eval TMP := $(shell mktemp -d))  and then try echo $(TMP)  
>> SHELL/BASH put a command into a variable with quotes and then execute it:  DEFINE: MYCMD=(perl -ne 'print 1;')   THEN:  gzcat FILE | "{MYCMD[@]}"  
>> SHELL/BASH startup a 'fresh' shell with NO user configuration / clean startup / no .bashrc / no init:  env -i bash --norc --noprofile       
>> SHELL/BASH script header (2 lines): #!/bin/bash __NEWLINE__ set -euo -o pipefail; shopt -s nullglob; shopt -s globstar; set -o xtrace;  
>> SHELL/BASH in MAKEFILE: In a Make rule, you can use a bash loop like so, with THREE slashes before a quote and SLASH-DOLLAR-DOLLAR instead of a dollar sign to access BASH variables. Example:  bash -c "for fff in */*; do echo -e \\\"\$$fff\\\"; done; echo -e \\\"This goes in a makefile.\\\"; "  
>> SHELL/BASH: Literal tab: $'\t' (example: join -t $'\t' to join on tabs) or $(echo -en '\t') . In MAKE: printf '\t'  
>> SHELL/BASH: Get directory of this script (fails if last item is a symlink): DIR_OF_SCRIPT="$( cd "$(dirname "${BASH_SOURCE[0]}" )" && pwd )"  
>> SHELL/BASH: Foreach/rename: for f in $(ls); do echo $f will become ${f/.txt/.newending} ; done  
>> SHELL/BASH: redirect STDERR & STDOUT both to console and to a file: THECMD 2>&1 | tee --append LOGFILE 
>> SHELL/BASH: See a function definition: type FUNCTIONNAME  
>> SHELL/BASH: Change tab width in bash:  setterm -regtabs 16 (16 = huge!)  
>> SHELL/BASH: Expand tabs/tab width: something | expand -t 32  
>> PERL: redirect STDERR and STDOUT in backticks: my $stderr_and_out = `commandhere 2>&1`; my $exitCode = $?;  
  
>> BIOINF / GTF: Sort a GTF file:  LC_ALL="C" sort -k 1,1 -k 4,4n YOUR.GTF > SORTED.GTF  
>> BIOINF / GTF: Load a GTF file into R:  require(rtracklayer); require(GenomicRanges); granges_from_gtf <- rtracklayer::import.gff('~/THE_GTF.GZ')   
  
>> Gnu parallel: works like xargs, but works with files with spaces, by default.  
>> XARGS/FIND multiple commands: find . -name SOMEFILE -print0 | xargs -0 -I {} sh -c "echo {}; thing {} | piped here | wc -l ; "   
>> XARGS/FIND: bzip all the bed/diff/count files that you can find: find . -type f \( -iname "*.bed" -o -iname "*_tracking" -o -iname "*.diff" -o -iname "*.moa" -o -iname "*.fasta" -o -iname ".fa" -o -iname "*.bed.count" \) -print0 | xargs -0 bzip2  
>> CHMOD Make files readable, directories r+x (print0/-0 makes filenames with spaces work): sudo chmod -R a+r ./ ; sudo find ./ -type d -print0 | sudo xargs -0 chmod a+x   (this is the command 'sudoshowoff') 
>> GNU FIND and DELETE a file (dangerous): find . -name SOMEFILE -delete  
>> PERL: fast sort with ONE header line: cat FILE | perl -e 'print scalar (<>); print sort <>' > OUTFILE  
>> PERL: REGEXP multi-line search/replace: perl -00pe 's{thing1}{thing2}gxms' THE_FILE  <-- s lets '.' match newlines\; m makes ^ and $ work  
>> PERL: replace IN PLACE in a file, with backups for $file in FILES; do perl -p -i.bak -e 's/STRING_TO_REPLACE/NEW_STRING/g' $file; done   
>> PERL: or WEB: Chrome web form non-breaking space (ASCII 160) is [\xA0] (or 'use feature qw(unicode_strings);')  
>> PERL: CPAN invocation: sudo perl -MCPAN -e shell  
>> PERL: CPAN upgrade all WITHOUT prompting: PERL_MM_USE_DEFAULT=1 && sudo perl -MCPAN -e shell >> then o conf build_requires_install_policy yes  ; o conf prerequisites_policy 'follow'  ; o conf commit ;  upgrade /(.\*)/"  
>> PERL: Backreferences/does something show up twice on a line: perl -n -e 'print (($_ =~ /\b(Something\d+)\s.\g1/) ? qq{match: $1\n} : qq{Nope\n});'  
>> EMACS: insert verbatim newline in search-and-replace:  Ctrl-Q Ctrl-J (this is also how you insert a newline when editing in iPython / PDB / python debugging on the command line  
>> EMACS: install new packages: list-packages  
>> EMACS: show variable: describe-variable  
  
>> Convert SAM to BAM with QPLZ:  for SSS in **/*.sam; do qplz --background "samtools view -bS $SSS > ${SSS/.sam}.bam" ; done  
>> BIOINF FASTA remove linebreaks from sequence (make one-line fasta sequences): cat file.fasta | awk '!/^>/{printf "%s", $0; n="\n"} /^>/{print n $0; n=""} END {printf"%s",n}' > out.fa  
>> BIOINF SEQ: fastq2fasta (fq2fa, fastq -> fasta): zcat FQ.fq.gz | awk '{if(NR%4==1) {printf(">%s\n",substr($0,2));} else if(NR%4==2) print;}' > FASTA.fa  
>> BIOINF SEQ: bcl2fastq: NCPU=4; bcl2fastq --fastq-compression-level 5 --barcode-mismatches 1 --no-lane-splitting -r  -d  -p  -w  --sample-sheet 'THIS_MUST_BE_VALID_NOT_MAC_FORMAT.csv'  --runfolder-dir=WEIRDLY_NAMED_160_NB01415_ABCDQBXX   (output goes to Data/Intensities/BaseCalls/*.fastq.gz)  
>> BIOINF FASTQ/FASTA subsample 500 reads randomly. Use -s1234 (random seed) so you can get the same reads in paired-end situations: seqtk sample -s1234 read1.fq.gz 500 | gzip > sub1.fq.gz  
>> BIOINF FASTQ: sort by read ID:  zcat YOUR.fq.gz | paste - - - - | sort -k1,1g -t " " | tr "\t" "\n" | gzip > SORTED.fq.gz (or use 'fastq_sort_by_read_name.sh')  
>> BIOINF RNASEQ: SAM --> BAM:  samtools view -bS in.sam > out.bam  
>> BIOINF RNASEQ: BAM --> SAM:  samtools view -h  in.bam > out.sam  
>> BIOINF RNASEQ: samtools view first read of pair only:  samtools view -f  64 file.bam  
>> BIOINF RNASEQ: samtools view second read of pair only: samtools view -f 128 file.bam  
>> BIOINF RNASEQ: sort SAM and setting the 'sorted' flag:  java -Xmx2g  -jar SortSam.jar INPUT=in.sam SORT_ORDER=coordinate OUTPUT=out_sorted.sam  
>> BIOINF RNASEQ: View BAM header: samtools view -H  in.bam  
>> BIOINF RNASEQ: Merge BAM files in subdirectories: for f in * ; do samtools merge ${f}_merged.bam $f/*.bam; done  
>> BIOINF RNASEQ: arbitrary text filtering in Perl on a BAM file: samtools view -h in.bam | perl -ne 'if (m/^@/) {print;} else { print if m/YOURCONDITIONHERE/; }' | samtools view -bS > filtered.bam   
>> BIOINF RNASEQ: remove non-primary and non-aligned reads: samtools view -b -F 0x104 in.bam > mapped_primary.bam  
>> BIOINF RNASEQ: count primary (0x100) mapped (0x4) reads: samtools view -c -F 0x104 in.bam > COUNT.txt 
>> BIOINF RNASEQ: Count frequencies of first 5 bases of a SAM file: cut -f 10 theFile.sam | cut -c 1-5 | sort | uniq -c  
>> BIOINF RNASEQ: TOPHAT/rename subdirectories TEST (change 'echo' to 'mv' for the real one): for f in *; do echo ${f}/accepted_hits.bam ${f}/accepted_hits_${f}.bam ; done  
>> BIOINF RNASEQ: Truncate FASTQ file to length 19 without fastx-trimmer: awk 'NR % 2 == 0 { $0=substr($0,1,19)} {print}'  
>> BIOINF SRA: Obtain SRA sequence from NCBI: prefetch --verbose SRR__whateverID__  
>> BIOINF SRA->FASTQ: fastq-dump --gzip SRR1234.sra    (<-- creates new file 'SRR1234.fastq.gz')  
>> MINION LONG READ SEQ: FAST5 to FASTQ: Make a single FASTQ out of all FAST5s in each of the 'barcode' subdirectories (assumes the file structure is: minion/barcode01/batch9988/file.fast5): for bar in barcode*; do final=ToFQ.$bar.final.fq.gz; /bin/rm -f $final; for f in ${bar}/*/*.fast5; do echo "Handling $f"; poretools fastq $f | gzip >> $final; done; done  
  
>> BIOINF BLAST: Extract all info (sequence names and FASTA sequence) from a database: blastdbcmd -entry all -db YOUR_DATABASE_PREFIX  
>> BIOINF BLAST: DNA pair-wise similarity for all sequences in a fasta file:   makeblastdb -in REF.fa -dbtype nucl  &&  blastn -outfmt 6 -db REF.fa -query REF.fa -title 'BLAST index' -out results.out  
>> BIOINF BLAST: MakeblastDB on COMPRESSED gzip input fasta: gunzip -c GENOME.fasta.gz | makeblastdb -in - -title 'OUTPUTNAME' -dbtype nucl -out OUTFILEPREFIX   
>> BIOINF BLAST: extract DATABASE sequences back to FASTA: In the directory with database files starting with the prefix 'nt_your_prefix_whatever', blastdbcmd -db nt_your_prefix_whatever -entry all -out - | gzip > all_sequences.fasta   
>> BIOINF BLAST: if you have PROTEINS, see where they are in NUCLEOTIDE space using tblastn. Also use a custom output format that SHOWS the alignment in a table: tblastn -query myproteins.faa -subject refgenome.fasta -outfmt "6 qacc sacc btop" > OUTPUT_with_alignment.blast.out.txt  
>> BIOINF BLAST: if you generate a blast query with -outfmt 11, you can use 'blast_formatter' to reformat it into other output types! So:   blast [...] -outfmt 11 -out BLASTOUT.asn     and then:    blast_formatter -archive BLASTOUT.asn -outfmt "7 qacc sacc evalue qstart qend sstart send "   
>> BIOINF BLAST: the ultimate custom formatting for thorough individuals: blastn -query QUERY.fa -subject REFERENCE.fa -outfmt "7 qseqid qlen qstart qend sseqid slen sstart send qseq sseq evalue bitscore score length pident nident mismatch gaps btop" > outfmt_custom.blast.txt   
>> BIOINF DNA reverse-complement: perl -e '$x = qq{AAAA_SEQUENCE_GGGG_TT_CC}; $_ = scalar(reverse($x)); tr/ACGT/TGCA/; print $_;'  
  
>> UNIX APACHE error log: sudo less -S /var/log/apache2/error.log  
>> MAC: DNS woes: sudo dscacheutil -flushcache  
>> MAC: Dictionary: /usr/share/dict/words  
>> MAC: why isn't the screensaver working: pmset -g (see 'sleep' in there), then pmset -g assertions  
>> MAC: Image type convert: mkdir -p PNGS_TO_JPEG; sips -s format jpeg *.png --out PNGS_TO_JPEG  
>> UNIX COPROC: coproc: re-run a command every 19 seconds: for i in {1..50} ; do coproc { ls >> myfile.tmp ; } ; sleep 19; kill $COPROC_PID ; sleep 2; done  
>> LINES: RANDOM subset (0.5 = 50%, 0.1 = 10%, etc):  perl -ne 'print if (rand() < 0.5);' theFile.txt  
>> LINES: REPEATABLE RANDOM subset: perl -e 'srand(123456); while(<>){ print if (rand() < 0.5);}' theFile.txt  
>> LINES: Semi-random FASTA records (2 lines at a time): perl -e 'my $total=0; my $MAX=3; srand(); while(<>){ if (rand()<0.01) { print $_; my $nextLine=<>; print $nextLine; $total++; exit(0) if ($total>=$MAX); } }'  
>> LINES: Range from N to M, inclusive (starts at 1):  sed -n N,Mp INPUTFILE  or  for a HUGE file: sed -n '(N+1)q;N,Mp' <-- quit on line (N+1)  
>> LINES: Every Nth line, starting with line Y (starts at 1, not 0): awk 'NR%X==Y' FILE. So getting line 1,3,5,7,9 is awk 'NR%2==1'. FASTQ example: sequences only is awk 'NR%4==2' 
>> LINES/AWK: Count num columns in file: awk -F '\t' '{print NF}'  
>> LINES based on perl expr: perl -e '$n = 0; while(<>) { if ($n%2 == 0) { print $_; }; $n++; }'  
>> LINES Count chars on each line: cat FILENAME | awk '{print length($0)}'  
>> UNIX/LINES/AWK re-order columns: cat FILE | awk 'BEGIN {OFS = "\t"}; {print $1,$4,$2,$11}'  
>> SQL: safely typecast / type cast / convert type / coerce varchar to int without error. You can cast the candidate variable to a FLOAT first. Do not use regular expressions! : select (CASE WHEN ABS('-9999999999999999999999999999999'::float) >= 2^63 THEN 'too large' ELSE '-99....9999'::bigint END). Note that the first cast is to a FLOAT, despite the final cast being a bigint here. This is intentional. 
>> SQL/REDSHIFT: Calculate table size (MB, GB, rows): SELECT "table", (size/1024.0)::numeric(10,1) as size_gb, (tbl_rows/1000000.0)::numeric(30,2) as rows_millions FROM SVV_TABLE_INFO; --redshift cmd for checking table size  
>> SQL/PSQL/POSTGRES:   SELECT current_timestamp AT TIME ZONE 'america/los_angeles' AS your_time_column;     
>> SQL/PSQL/POSTGRES:   WITH a2 as (SELECT * FROM (SELECT *,ROW_NUMBER() OVER (partition by store,item,date,whatever order by date) FROM yourtable) zzz WHERE row_number=1) -- ensure that you only get ONE RESULT (one row) per select statement!      
>> SQL/PSQL/POSTGRES:   from the RDB, access the DWH. Works directly in the RDB, so you can say 'WITH XYZ as (...)', for example. SELECT * FROM public.dblink('redshift_server', 'SELECT * FROM cool_schema.favorite_table') t1(a int, b varchar, c int8);  ;     
>> SQL/PSQL/POSTGRES:   GRANT USAGE on SCHEMA your_cool_schema to PUBLIC;    GRANT SELECT on ALL TABLES IN SCHEMA your_cool_schema TO PUBLIC;   ALTER DEFAULT PRIVILEGES IN SCHEMA your_cool_schema GRANT SELECT ON TABLES TO PUBLIC;     
>> SQL/PSQL/POSTGRES: Get a week number for a year, with the year too, e.g. 2019-51: select to_char(SOME_DATE_HERE, 'YYYY-WW')  (for month, use YYYY-MM)  
>> SQL/PSQL/POSTGRES: Get a single value on the command line: 1. make sure to set your ~/.pgpass to put your password in, so you don't have to type it, and then: psql -q -t -A -X -h SERVERHERE -U USERHERE -d DBNAME -p PORTNUMBER -c 'select count(*) from sometable;'  
>> SQL/PSQL/POSTGRES: Just connect interactively: psql -h SERVERHERE -U USERHERE -d DBNAME -p PORTNUMBER -e ; <interactive password prompt will occur after you type that>  
>> SQL/POSTGRES Amazon Redshift show DATA LOADING errors: BEST SIMPLE WAY: select * from stl_load_errors e order by starttime desc;       Maybe try this to only get the MOST RECENT query:   SELECT * FROM stl_load_errors WHERE query IN (select max(query) from stl_load_errors) ORDER BY starttime DESC limit 99;     More complex way that sometimes does not show everything: select d.*,e.* from stl_load_errors e, stl_loaderror_detail d where d.query = e.query order by starttime desc; 
>> SQL/POSTGRES date to human-readable DOW / day of week (does NOT work with text, you must cast it to ::date): TO_CHAR(current_date, 'day') or SELECT TO_CHAR(current_date, 'DY') --> result is e.g. 'MON';  If you want the DAY OF WEEK (and not the week number), try DATE_PART('dow',current_date) <-- numeric. Beware of 1 or 0 starting the week--it is different from Python!    

>> SQL postgres ONLY (command line):  \dt SCHEMANAME.* -- show all tables in a schema   \x: print rows-as-columns. \z: show current schema. \d schemaname.tablename: describe table.

>> SQL/POSTGRES get SECOND highest (Nth item) item in a column (memory-inefficient): SELECT  (ARRAY_AGG(colname ORDER_BY colname DESC))[2] FROM yourtable;    
>> SQL/POSTGRES list databases: SELECT datname FROM pg_database WHERE datistemplate=FALSE;    
>> SQL/POSTGRES list databases, schemas and tables: SELECT table_schema,table_name  FROM information_schema.tables  ORDER BY table_schema,table_name;   
>> SQL/POSTGRES create a temp table with constants:  CREATE TEMP TABLE settings AS SELECT 109 as store, 3454 as item;  
>> SQL/POSTGRES count records grouped by month and year:   SELECT EXTRACT(YEAR from the_date) as YYYY, EXTRACT(MONTH from the_date) as MM, count(*) as count FROM YOUR_DATA WHERE 1=1 group by EXTRACT(MONTH from date), EXTRACT(YEAR from date) ORDER BY YYYY asc, MM asc;  
>> SQL/POSTGRES copy (duplicate) a table: CREATE TABLE new AS (SELECT * FROM old);  
>> SQL/POSTGRES remove dupes from a table. Tested in Redshift   <NL> ROLLBACK; BEGIN TRANSACTION;  <NL> CREATE TABLE yourschema.without_dupes AS  <NL> (WITH ddd AS (select *,ROW_NUMBER() OVER (PARTITION BY field1,field2,etc,even,more,fields ORDER BY some_field_maybe DESC) row_number FROM yourschema.table_to_dedupe)  <NL>SELECT field1,field2,every,field,EXCEPT,FOR,do_not_select_row_number_here FROM ddd WHERE row_number=1););  <NL> ALTER TABLE yourschema.original_table RENAME TO original_table_backup;  <NL> ALTER TABLE deduplicated_table RENAME TO original_table;  <NL> COMMIT;  
>> Python read/write from file in one line (OVERWRITES--does not append):  from pathlib import Path   //    Path('out.txt').write_text('output filename to OVERWRITE')  // To read from a file:  from_file = Path('file.txt').read_text()        To APPEND from a file, you need two lines: with Path('file.txt').open('a') as f: // f.write('...appending') 
>> PYTHON eradicate virtual environment and start from scratch / restart venv: deactivate && cd ~/core && rm -rf the_env && python3 -m venv ./the_env && source the_env/bin/activate && make install (assumes certain locations for, e.g. 'activate' and 'the_env'  
>> PYTHON/PANDAS: get count/index of UNIQUE VALUES in a list:  labels,actual_values = pd.factorize(your_dataframe['class']) (labels start at 0)  
>> PYTHON/PANDAS: get count/index of UNIQUE VALUES in a list:  labels,actual_values = pd.factorize(your_dataframe['class']) (labels start at 0)  
>> PYTHON/PANDAS need to access the INDEX columns in a pandas dataframe? Here's the super elegant and not overly verbose way:  forecast_init_df.index.get_level_values('ds')   
>> PYTHON/PANDAS aggregate (.agg()) (group by / group_by / group by) multiple times over the same column, saving to new named columns: by_state = all_usa_city_data.groupby('state').agg(num_cities=pd.NamedAgg(column='cityname', aggfunc='count'), state_bird=pd.NamedAgg(column='state_bird', aggfunc='first'), state_pop_all_cities=pd.NamedAgg(column='citypop', aggfunction='sum')).reset_index()  
>> PYTHON/PANDAS APPEND columns as a chained command with column name in a variable: new_df   = (pd.DataFrame(list(some_stuff), columns=['ds']).assign(**{NEW_COLUMN_VARIABLE_NAME:0}) )  
>> PYTHON/PANDAS sort pandas data frame in place by absolute value: DF.reindex(DF['your_value'].abs().sort_values(ascending=False).index)    
>> JUPYTER/PYTHON output to HTML just like you see it on screen:   jupyter nbconvert --output-dir=~/Desktop/ --to 'html' --ExecutePreprocessor.timeout=1800 --execute YOUR_NOTEBOOK_NAME_HERE.ipynb   
>> JUPYTER/PYTHON prevent auto-closing brackets/braces/parens: from notebook.services.config import ConfigManager  <NEWLINE>  c = ConfigManager()   <NEWLINE>      c.update('notebook', {'CodeCell': {'cm_config': {'autoCloseBrackets': False}}})    
>> JUPYTER/PYTHON prevent scrolling regions in plots. Put this in its own cell: %%javascript  <NEWLINE>   IPython.OutputArea.prototype._should_scroll = function(lines) { return false; }    
>> JUPYTER/PYTHON/MATPLOTLIB/SNS change figure size plt.rcParams['figure.figsize'] = (WIDTH, HEIGHT)   and   plt.rcParams['figure.dpi'] = 250     
>> JUPYTER/PYTHON output raw formatted text as HTML, programmatically:   display(Markdown('<SPAN style='color:green; font-size:8px;'>Your text here</SPAN>'))  
>> Z SOFTWARE to remember: Bioinformatics: UGENE will do Clustal/BLAST/Etc locally, and visualize sequences / FASTAS. MAUVE is a similar aligner that we used previously for viral genomes.  IGV is a local genome browser. TreeView and Cluster 3 will cluster.  General: GLANCES is fancier top/htop system activity monitor. HEXYL is a colorized Mac command line hex editor (brew install hexyl). 
>> Tableau 'roll up' (aggregate / groupby / group by) fields by some other field: {INCLUDE [state_of_usa] : SUM([cars_sold_per_county])} / {INCLUDE [state_of_usa] : SUM([num_people_per_county])}  
>> Tableau 'maximum' date/value across a table {FIXED : MAX( { FIXED [YOUR_GROUPING_1] : MAX( [THE_VALUE_TO_MAX] ) } ) }  
>> Tableau: get the most recent date in a giant table with a bunch of fields:  (new calculated field) {FIXED [SomeTopLevelCategory] : MAX([Date])}  
>> Tableau: get a few days of data: window_sum(sum([your_cool_daily_value]), -4, -1)  // [-4 to -1] is the preceding 3 days, but not today   
>> Tableau: custom date format for e.g. 2001-12-11 (Mon):  yyyy-MM-dd (ddd)  
>> Tableau: look up something in a date range to find earlier/later values: LOOKUP(ZN(SUM([inventory_today])), 7) // the '7' would mean '7 days in the future'. The <SUM> part is mandatory even for a non-aggregate variable.  
>> TABLE VIEWER 'vd' from homebrew: vd --filetype='tsv' --delimiter='|' YOURFILE.csv   (CTRL-b and CTRL-f to move up/down a screen)  
>> GOOGLE SHEETS (maybe also Excel): count if two columns (K and L here) are EQUAL and the first one is non-blank: =SUM(ArrayFormula(NOT(ISBLANK(K12:K999)) * (K12:K999 = L12:L999)))  
>> GOOGLE SHEETS: combine TWO sheets of unknown length: =QUERY({cool_sheet!A4:F;even_cooler_sheet!A5:F},{"SELECT Col1,Col2,Col3,Col4,Col5,Col6 WHERE Col1 != ''"}, 0)  # the 0 means 'zero header lines'    
>> SSH complaining about permissions:  ssh-add -K ~/.ssh/id_rsa (on the Mac--should persist between reboots) or ssh-add ~/.ssh/id_rsa   should probably fix it permanently (rather than requiring 'ssh-add' to be run every time you launch a new terminal)  
>> BREW uninstall an application that was manually deleted from Applications: e.g. (brew uninstall --cask zettlr  would remove /Applications/Zettlr.app) -- this solves the brew upgrade message "It seems the App source [...] is not there"

>> ITERM2 text highlighting with partial matches / highlight only a subset of matched text / highlight partial match (lookback / lookahead regexp): if you want to highlight only AND, and only when it appears as "THIS AND THAT": you would add a rule for (?<=THIS )AND(?= THAT) <-- only the AND will be highlighted by iTerm.

>> DEDUPE / DEDUPLICATE / REMOVE DUPLICATE FILES: use the utility 'fdupes'

END_OF_HEREDOC
       ) # This end-paren is important! It matches the one above
    echo "$VERBATIM_CHEAT_SHEET"
}

